{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/your-repo/your-project/blob/main/v2/nb/process_raw_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"M25gQ5m7zC9h","executionInfo":{"status":"ok","timestamp":1740057337569,"user_tz":300,"elapsed":2655,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"source":["# Mount Google Drive to access data\n","from google.colab import drive, runtime\n","# Import required modules\n","import os\n","import glob\n","import time\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler, RobustScaler\n","import json\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"f6_JNbBrx48E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740057359964,"user_tz":300,"elapsed":22392,"user":{"displayName":"Ben","userId":"16626926592568551176"}},"outputId":"e547fac4-651b-4b25-f3c0-08656c656979"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\"\"\"Main function to process all years in two steps.\"\"\"\n","# Configuration\n","BASE_INPUT_DIR = '/content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed'\n","BASE_OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed_scaled'\n","# YEARS = [2019, 2020, 2021, 2022, 2023, 2024]\n","YEARS = [2024]\n","SCALER_TYPE = 'standard'  # 'standard' or 'robust'\n","\n","# Feature names for reference and logging\n","NUMERIC_FEATURES = [\n","    'LST_Day_1km', 'LST_Night_1km',\n","    'Emis_31', 'Emis_32', 'dewpoint_temperature_2m',\n","    'temperature_2m', 'soil_temperature_level_1',\n","    'surface_net_thermal_radiation', 'u_component_of_wind_10m',\n","    'v_component_of_wind_10m', 'surface_pressure', 'total_precipitation',\n","    'elevation', 'NDVI'\n","]"],"metadata":{"id":"CcaexO2XZnIK","executionInfo":{"status":"ok","timestamp":1740057359981,"user_tz":300,"elapsed":14,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def load_and_combine_features(input_dir, batch_pattern=\"batch_*.parquet\"):\n","    \"\"\"Load all features from batch files for scaling computation.\"\"\"\n","    print(f\"\\nLoading features from {input_dir}\")\n","    all_features = []\n","\n","    batch_files = glob.glob(os.path.join(input_dir, batch_pattern))\n","    total_files = len(batch_files)\n","\n","    for i, batch_file in enumerate(batch_files, 1):\n","        print(f\"Processing batch {i}/{total_files}: {os.path.basename(batch_file)}\")\n","        df = pd.read_parquet(batch_file)\n","        features = df[NUMERIC_FEATURES].values\n","        all_features.append(features)\n","\n","    return np.concatenate(all_features, axis=0)\n","\n","def compute_scaling_parameters(features, scaler_type='robust'):\n","    \"\"\"Compute scaling parameters for the features.\"\"\"\n","    if scaler_type == 'robust':\n","        scaler = RobustScaler()\n","    else:\n","        scaler = StandardScaler()\n","\n","    scaler.fit(features)\n","\n","    # Extract and format scaling parameters\n","    if scaler_type == 'robust':\n","        params = {\n","            'center_': scaler.center_.tolist(),\n","            'scale_': scaler.scale_.tolist()\n","        }\n","    else:\n","        params = {\n","            'mean_': scaler.mean_.tolist(),\n","            'scale_': scaler.scale_.tolist()\n","        }\n","\n","    # Add feature names to parameters\n","    params['features'] = NUMERIC_FEATURES\n","    params['scaler_type'] = scaler_type\n","\n","    return scaler, params\n","\n","def transform_and_save_batch(batch_file, scaler, output_dir):\n","    \"\"\"Transform a single batch file and save the result.\"\"\"\n","    df = pd.read_parquet(batch_file)\n","\n","    # Extract features for transformation\n","    features = df[NUMERIC_FEATURES].values\n","\n","    # Transform features\n","    transformed_features = scaler.transform(features)\n","\n","    # Replace original features with transformed ones\n","    for i, col in enumerate(NUMERIC_FEATURES):\n","        df[col] = transformed_features[:, i]\n","\n","    # Save transformed batch\n","    output_file = os.path.join(output_dir, os.path.basename(batch_file))\n","    df.to_parquet(output_file)\n","\n","    return output_file\n","\n","def collect_yearly_statistics(input_dir, scaler_type='robust'):\n","    \"\"\"Collect statistical information for a single year.\"\"\"\n","    year = os.path.basename(input_dir)\n","    print(f\"\\nCollecting statistics for year: {year}\")\n","\n","    # Load and combine features for scaling\n","    features = load_and_combine_features(input_dir)\n","\n","    # Compute scaling parameters without transforming\n","    print(\"Computing scaling parameters...\")\n","    scaler, scaling_params = compute_scaling_parameters(features, scaler_type)\n","\n","    # Add year information\n","    scaling_params['year'] = year\n","    scaling_params['n_samples'] = len(features)\n","\n","    return scaling_params\n","\n","def combine_yearly_statistics(yearly_stats):\n","    \"\"\"Combine statistics from multiple years to create global scaling parameters.\"\"\"\n","    if not yearly_stats:\n","        raise ValueError(\"No yearly statistics provided\")\n","\n","    # Get feature names and scaler type from first year\n","    features = yearly_stats[0]['features']\n","    scaler_type = yearly_stats[0]['scaler_type']\n","\n","    if scaler_type == 'robust':\n","        # For RobustScaler, combine centers and scales weighted by number of samples\n","        total_samples = sum(stats['n_samples'] for stats in yearly_stats)\n","        combined_center = np.zeros(len(features))\n","        combined_scale = np.zeros(len(features))\n","\n","        for stats in yearly_stats:\n","            weight = stats['n_samples'] / total_samples\n","            combined_center += np.array(stats['center_']) * weight\n","            combined_scale += np.array(stats['scale_']) * weight\n","\n","        combined_params = {\n","            'center_': combined_center.tolist(),\n","            'scale_': combined_scale.tolist()\n","        }\n","    else:\n","        # For StandardScaler, combine means and variances\n","        total_samples = sum(stats['n_samples'] for stats in yearly_stats)\n","        combined_mean = np.zeros(len(features))\n","        combined_var = np.zeros(len(features))\n","\n","        # First pass: combine means\n","        for stats in yearly_stats:\n","            weight = stats['n_samples'] / total_samples\n","            combined_mean += np.array(stats['mean_']) * weight\n","\n","        # Second pass: combine variances\n","        for stats in yearly_stats:\n","            weight = stats['n_samples'] / total_samples\n","            combined_var += (np.array(stats['scale_']) ** 2) * weight\n","\n","        combined_params = {\n","            'mean_': combined_mean.tolist(),\n","            'scale_': np.sqrt(combined_var).tolist()\n","        }\n","\n","    # Add metadata\n","    combined_params['features'] = features\n","    combined_params['scaler_type'] = scaler_type\n","    combined_params['total_samples'] = total_samples\n","\n","    return combined_params\n","\n","def create_scaler_from_params(params):\n","    \"\"\"Create a scaler object from parameters.\"\"\"\n","    if params['scaler_type'] == 'robust':\n","        scaler = RobustScaler()\n","        scaler.center_ = np.array(params['center_'])\n","        scaler.scale_ = np.array(params['scale_'])\n","    else:\n","        scaler = StandardScaler()\n","        scaler.mean_ = np.array(params['mean_'])\n","        scaler.scale_ = np.array(params['scale_'])\n","\n","    return scaler"],"metadata":{"id":"IR85UzAmZtD4","executionInfo":{"status":"ok","timestamp":1740057360164,"user_tz":300,"elapsed":179,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(f\"Starting data scaling process at {datetime.now()}\")\n","print(f\"Input directory: {BASE_INPUT_DIR}\")\n","print(f\"Output directory: {BASE_OUTPUT_DIR}\")\n","print(f\"Scaler type: {SCALER_TYPE}\")\n","\n","os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n","combined_stats_file = os.path.join(BASE_OUTPUT_DIR, 'combined_scaling_params.json')\n","\n","# Check if combined scaling parameters already exist\n","if os.path.exists(combined_stats_file):\n","    print(f\"\\nFound existing scaling parameters at: {combined_stats_file}\")\n","    with open(combined_stats_file, 'r') as f:\n","        combined_stats = json.load(f)\n","else:\n","    # Step 1: Collect statistics from all years\n","    print(\"\\nStep 1: Collecting statistics from all years...\")\n","    yearly_stats = []\n","\n","    for year in YEARS:\n","        input_dir = os.path.join(BASE_INPUT_DIR, str(year))\n","        if not os.path.exists(input_dir):\n","            print(f\"Warning: Directory for year {year} not found, skipping...\")\n","            continue\n","\n","        year_stats = collect_yearly_statistics(input_dir, SCALER_TYPE)\n","        yearly_stats.append(year_stats)\n","\n","    # Combine statistics from all years\n","    print(\"\\nCombining statistics from all years...\")\n","    combined_stats = combine_yearly_statistics(yearly_stats)\n","\n","    # Save combined statistics\n","    with open(combined_stats_file, 'w') as f:\n","        json.dump(combined_stats, f, indent=2)\n","    print(f\"Saved combined scaling parameters to: {combined_stats_file}\")\n","\n","# Step 2: Apply scaling to each year using combined statistics\n","print(\"\\nStep 2: Applying scaling to each year...\")\n","scaler = create_scaler_from_params(combined_stats)\n","\n","for year in YEARS:\n","    input_dir = os.path.join(BASE_INPUT_DIR, str(year))\n","    if not os.path.exists(input_dir):\n","        continue\n","\n","    output_dir = os.path.join(BASE_OUTPUT_DIR, str(year))\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    print(f\"\\nProcessing year: {year}\")\n","    start_time = time.time()\n","\n","    # Process each batch file\n","    batch_files = glob.glob(os.path.join(input_dir, \"batch_*.parquet\"))\n","    total_batches = len(batch_files)\n","\n","    print(f\"Processing {total_batches} batch files...\")\n","    for i, batch_file in enumerate(batch_files, 1):\n","        print(f\"Transforming batch {i}/{total_batches}: {os.path.basename(batch_file)}\")\n","        transform_and_save_batch(batch_file, scaler, output_dir)\n","\n","    # Copy location mapping file if it exists\n","    mapping_file = os.path.join(input_dir, 'location_mapping.parquet')\n","    if os.path.exists(mapping_file):\n","        output_mapping = os.path.join(output_dir, 'location_mapping.parquet')\n","        pd.read_parquet(mapping_file).to_parquet(output_mapping)\n","\n","    duration = time.time() - start_time\n","    print(f\"Year {year} processing completed in {duration:.2f} seconds\")\n","\n","print(f\"\\nScaling process completed at {datetime.now()}\")\n","print(f\"Scaled data saved to: {BASE_OUTPUT_DIR}\")\n","print(f\"Combined scaling parameters saved to: {combined_stats_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qh1dYsxbZuAj","outputId":"1ccfb116-80e8-4a32-d5cb-8d665aab72c9","executionInfo":{"status":"ok","timestamp":1740057704838,"user_tz":300,"elapsed":344676,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting data scaling process at 2025-02-20 13:16:00.127372\n","Input directory: /content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed\n","Output directory: /content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed_scaled\n","Scaler type: standard\n","\n","Found existing scaling parameters at: /content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed_scaled/combined_scaling_params.json\n","\n","Step 2: Applying scaling to each year...\n","\n","Processing year: 2024\n","Processing 16 batch files...\n","Transforming batch 1/16: batch_0001.parquet\n","Transforming batch 2/16: batch_0002.parquet\n","Transforming batch 3/16: batch_0003.parquet\n","Transforming batch 4/16: batch_0004.parquet\n","Transforming batch 5/16: batch_0005.parquet\n","Transforming batch 6/16: batch_0006.parquet\n","Transforming batch 7/16: batch_0007.parquet\n","Transforming batch 8/16: batch_0008.parquet\n","Transforming batch 9/16: batch_0009.parquet\n","Transforming batch 10/16: batch_0010.parquet\n","Transforming batch 11/16: batch_0011.parquet\n","Transforming batch 12/16: batch_0012.parquet\n","Transforming batch 13/16: batch_0013.parquet\n","Transforming batch 14/16: batch_0014.parquet\n","Transforming batch 15/16: batch_0015.parquet\n","Transforming batch 16/16: batch_0016.parquet\n","Year 2024 processing completed in 343.96 seconds\n","\n","Scaling process completed at 2025-02-20 13:21:44.789252\n","Scaled data saved to: /content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed_scaled\n","Combined scaling parameters saved to: /content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed_scaled/combined_scaling_params.json\n"]}]},{"cell_type":"code","source":["runtime.unassign()"],"metadata":{"id":"8mOujxjXxY6O","executionInfo":{"status":"ok","timestamp":1740057705011,"user_tz":300,"elapsed":170,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":6,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}