{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyPFjoJJG50Mo0OBtLEVo7OC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive, runtime\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NGpI_I5CWN9_","executionInfo":{"status":"ok","timestamp":1740765463647,"user_tz":300,"elapsed":24436,"user":{"displayName":"Ben","userId":"16626926592568551176"}},"outputId":"19b87d64-d537-49ce-d553-5cfcc9dab5ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import time\n","from datetime import datetime\n","import torch\n","from torch import optim\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","from sklearn.metrics import precision_recall_curve, auc, recall_score, precision_score, f1_score\n","# import wandb\n","from tqdm import tqdm\n","import gc\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","%matplotlib inline\n","\n","# Add autoreload support\n","%load_ext autoreload\n","%autoreload 2\n"],"metadata":{"id":"nW56_ISKXddQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","parent_dir = '/content/drive/My Drive/Colab Notebooks/wildfire'\n","sys.path.append(parent_dir)\n","\n","# Change to the working directory\n","os.chdir(os.path.join(parent_dir, 'v4'))\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HczMCOuXbCjn","executionInfo":{"status":"ok","timestamp":1740765471809,"user_tz":300,"elapsed":534,"user":{"displayName":"Ben","userId":"16626926592568551176"}},"outputId":"617c88b6-6e6d-4e5c-f2e9-022bd44577b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/wildfire/v4\n"]}]},{"cell_type":"code","source":["from v4.model.dataset_v4 import create_dataloaders, create_test_loader, create_baseline_dataloaders, create_baseline_test_loader, create_multi_target_dataloaders, create_multi_target_test_loader, create_multi_target_baseline_dataloaders, create_multi_target_baseline_test_loader\n","from v4.model.models_v4 import FireTransformer, BaselineModels, NNBaselineModels\n","from v4.model.loss_v4 import improved_weighted_focal_loss_v1, improved_weighted_focal_loss_v2, weighted_focal_loss\n","from v4.model.utils import calculate_metrics"],"metadata":{"id":"B_NqEZ5WaPFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NEED_TEST_DATA = True\n","# Global Configuration\n","BASE_PATH = '/content/drive/My Drive/Colab Notebooks/wildfire'\n","TRAIN_DATA_PATH = os.path.join(BASE_PATH, 'new_data/train/sequences_y2019-2024_w15_o0_r4.npz')\n","VAL_DATA_PATH = None\n","TEST_DATA_PATH = os.path.join(BASE_PATH, 'new_data/test_detect/sequences_y2025-2025_w15_o0_r4.npz')\n","\n","model_type = 'transformer'\n","\n","# Model Parameters\n","TASK='predict'\n","N_LAYERS = 4\n","D_FF = 256  # Add feed-forward dimension as hyperparameter\n","TARGET_COL = 'targetY_o1_prob'  # New hyperparameter\n","DROPOUT = 0.4\n","TRAIN_RATIO=0.97\n","USE_FEATURE_NORM = True\n","USE_WARMUP_SCHEDULER = False\n","BASE_POS_WEIGHT = 4\n","FN_PENALTY = 30.0\n","FP_PENALTY = 3\n","MAX_NORM=1.0\n","CONFIDENCE_MARGIN = 0.1\n","CONFIDENCE_WEIGHT = 5.0\n","EPOCHS = 20\n","\n","D_MODEL = 128\n","N_HEADS = 8\n","# BATCH_SIZE = 2048\n","BATCH_SIZE = 5120\n","LEARNING_RATE = 0.0001 * (BATCH_SIZE / 512)\n","LOG_INTERVAL = 100\n","EVAL_ADD_NOISE = False\n","EVAL_NOISE_STD = 1e-3\n","IS_PROB_TARGET = '_prob' in TARGET_COL  # Automatically set based on target column\n","FOCAL_GAMMA = 0\n","\n","# Add new hyperparameter\n","LOSS_VERSION = 'v1'  # Options: 'v1' for original, 'v2' for new anti-clustering version\n","\n","# Extract target_type from TARGET_COL\n","if TARGET_COL.endswith('_o1_prob'):\n","    TARGET_TYPE = 'o1'\n","elif TARGET_COL.endswith('_o2_prob'):\n","    TARGET_TYPE = 'o2'\n","elif TARGET_COL.endswith('_o3_prob'):\n","    TARGET_TYPE = 'o3'\n","else:\n","    TARGET_TYPE = 'basic'  # Default for binary targets\n","\n","# Create model name with hyperparameters\n","TEST_OUTPUT_DIR = os.path.join(BASE_PATH, 'new_data/test_detect/predictions')\n","\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","# Create model name with hyperparameters\n","if model_type == 'transformer':\n","    MODEL_NAME = f\"transformer_{TASK}_{model_type}_L{N_LAYERS}_D{D_MODEL}_H{N_HEADS}_G{FOCAL_GAMMA}_PW{BASE_POS_WEIGHT}_FN{FN_PENALTY}_FP{FP_PENALTY}_T{TARGET_COL}_DR{DROPOUT}_BN{USE_FEATURE_NORM}_WU_{USE_WARMUP_SCHEDULER}_LOSS{LOSS_VERSION}\"\n","elif model_type == 'traditional':\n","    MODEL_NAME = f\"TRAD_PW{BASE_POS_WEIGHT}_FN{FN_PENALTY}_FP{FP_PENALTY}_{timestamp}\"\n","elif model_type == 'nn':\n","    MODEL_NAME = f\"NN_PW{BASE_POS_WEIGHT}_FN{FN_PENALTY}_FP{FP_PENALTY}_E{NN_EPOCHS}_LR{NN_LEARNING_RATE}_DO{NN_DROPOUT}_{timestamp}\"\n","elif model_type == 'all':\n","    MODEL_NAME = f\"ALL_PW{BASE_POS_WEIGHT}_FN{FN_PENALTY}_FP{FP_PENALTY}_E{NN_EPOCHS}_LR{NN_LEARNING_RATE}_DO{NN_DROPOUT}_{timestamp}\"\n","else:\n","    MODEL_NAME = f\"UNKNOWN_{timestamp}\"\n","\n","LOG_DIR = os.path.join(BASE_PATH, 'runs', timestamp + '_' + MODEL_NAME)\n","\n","# Create necessary directories\n","os.makedirs(LOG_DIR, exist_ok=True)"],"metadata":{"id":"O04RmbabWTVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIoBMhmkV_aA"},"outputs":[],"source":["def plot_training_history(losses, val_metrics, save_path):\n","    \"\"\"Plot training history\"\"\"\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n","\n","    # Plot training loss\n","    ax1.plot(losses)\n","    ax1.set_title('Training Loss')\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Loss')\n","\n","    # Plot validation metrics\n","    for metric in ['auc_pr', 'accuracy', 'recall', 'precision']:\n","        values = [metrics[metric] for metrics in val_metrics]\n","        ax2.plot(values, label=metric)\n","    ax2.set_title('Validation Metrics')\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('Score')\n","    ax2.legend()\n","\n","    plt.tight_layout()\n","    plt.savefig(save_path)\n","    plt.close()\n","\n","def evaluate(model, dataloader, device, add_noise=False, noise_std=1e-3, is_prob_target=False):\n","    \"\"\"Evaluate model on dataloader\"\"\"\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            logits = model(X)\n","            probs = torch.sigmoid(logits)\n","            all_preds.extend(probs.cpu().numpy())\n","            all_labels.extend(y.cpu().numpy())\n","\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","\n","    # Log prediction distribution around 0.5\n","    exact_half = np.mean(all_preds == 0.5) * 100\n","    near_half_range = 0.01\n","    near_half = np.mean((all_preds > 0.5 - near_half_range) &\n","                       (all_preds < 0.5 + near_half_range)) * 100\n","\n","    print(f\"\\nPrediction distribution analysis:\")\n","    print(f\"Predictions exactly 0.5: {exact_half:.2f}%\")\n","    print(f\"Predictions within Â±{near_half_range} of 0.5: {near_half:.2f}%\")\n","\n","    # Optional prediction stabilization\n","    if add_noise and exact_half > 0:\n","        print(f\"Adding noise (std={noise_std}) to break symmetry\")\n","        all_preds = all_preds + np.random.normal(0, noise_std, all_preds.shape)\n","        if np.all(all_preds == 0.5):\n","            print(\"WARNING: All predictions still exactly 0.5 after noise, forcing split\")\n","            all_preds = np.random.choice([0.49, 0.51], size=all_preds.shape)\n","\n","    return calculate_metrics(all_preds, all_labels, is_prob_target=is_prob_target)\n","\n","def analyze_training_history(runs_dir):\n","    \"\"\"Analyze and visualize training history from multiple runs\"\"\"\n","    from torch.utils.tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n","\n","    runs = []\n","    metrics = ['val/auc_pr', 'val/accuracy', 'val/recall', 'val/precision', 'val/f1']\n","\n","    # Collect data from all runs\n","    for run_dir in os.listdir(runs_dir):\n","        run_path = os.path.join(runs_dir, run_dir)\n","        if not os.path.isdir(run_path):\n","            continue\n","\n","        event_path = None\n","        for root, _, files in os.walk(run_path):\n","            for file in files:\n","                if file.startswith('events.out.tfevents'):\n","                    event_path = os.path.join(root, file)\n","                    break\n","\n","        if event_path:\n","            ea = EventAccumulator(event_path)\n","            ea.Reload()\n","\n","            # Extract hyperparameters from directory name\n","            params = {}\n","            for param in run_dir.split('_'):\n","                if any(param.startswith(p) for p in ['L', 'D', 'H', 'G', 'PW', 'FN', 'FP']):\n","                    key = param[0:2]\n","                    value = float(param[2:])\n","                    params[key] = value\n","\n","            # Collect metrics\n","            run_data = {'params': params, 'metrics': {}}\n","            for metric in metrics:\n","                if metric in ea.scalars.Keys():\n","                    events = ea.Scalars(metric)\n","                    run_data['metrics'][metric] = {\n","                        'steps': [e.step for e in events],\n","                        'values': [e.value for e in events]\n","                    }\n","\n","            runs.append(run_data)\n","\n","    # Plot metrics\n","    plt.figure(figsize=(20, 15))\n","    for i, metric in enumerate(metrics, 1):\n","        plt.subplot(3, 2, i)\n","        for run in runs:\n","            if metric in run['metrics']:\n","                label = '_'.join(f\"{k}{v}\" for k, v in run['params'].items())\n","                plt.plot(\n","                    run['metrics'][metric]['steps'],\n","                    run['metrics'][metric]['values'],\n","                    label=label\n","                )\n","        plt.title(metric.split('/')[-1].upper())\n","        plt.xlabel('Steps')\n","        plt.ylabel('Value')\n","        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","        plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(runs_dir, 'comparison.png'), bbox_inches='tight')\n","    plt.close()\n","\n","def train_transformer():\n","    \"\"\"Train Transformer model\"\"\"\n","    start_time = time.time()\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    writer = SummaryWriter(LOG_DIR)\n","\n","    # Early stopping parameters\n","    early_stopping_patience = 10\n","    early_stopping_counter = 0\n","    best_val_auc = 0\n","\n","    # Create dataloaders with validation data path\n","    print(\"\\nCreating dataloaders...\")\n","    dataloader_start = time.time()\n","    train_loader, val_loader = create_multi_target_dataloaders(\n","        TRAIN_DATA_PATH,\n","        target_col=TARGET_COL,\n","        batch_size=BATCH_SIZE,\n","        train_ratio=TRAIN_RATIO,\n","        val_data_path=VAL_DATA_PATH\n","    )\n","    print(f\"Dataloader creation took {time.time() - dataloader_start:.2f} seconds\")\n","\n","    # Check class distribution\n","    train_labels = np.concatenate([y.numpy() for _, y in train_loader])\n","    val_labels = np.concatenate([y.numpy() for _, y in val_loader])\n","    print(f\"Training class distribution: {np.bincount(train_labels.astype(int))}\")\n","    print(f\"Validation class distribution: {np.bincount(val_labels.astype(int))}\")\n","\n","    # Create model\n","    print(\"\\nInitializing model...\")\n","    model_start = time.time()\n","    feature_size = next(iter(train_loader))[0].shape[-1]\n","    window_size = next(iter(train_loader))[0].shape[1]  # Get window size from data\n","    model = FireTransformer(\n","        feature_size=feature_size,\n","        window_size=window_size,  # Pass window size from data\n","        n_layers=N_LAYERS,\n","        d_model=D_MODEL,\n","        n_heads=N_HEADS,\n","        dropout=DROPOUT,  # Add dropout parameter\n","        use_feature_norm=USE_FEATURE_NORM\n","    ).to(device)\n","    print(f\"Model initialization took {time.time() - model_start:.2f} seconds\")\n","\n","    # Setup training\n","    optimizer = optim.AdamW(\n","        model.parameters(),\n","        lr=LEARNING_RATE,\n","        weight_decay=0.01,  # Add weight decay for larger batch size\n","        betas=(0.9, 0.999)  # Default Adam betas usually work well\n","    )\n","\n","    if USE_WARMUP_SCHEDULER:\n","        scheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n","                                            max_lr=LEARNING_RATE,\n","                                            epochs=EPOCHS,\n","                                            steps_per_epoch=len(train_loader),\n","                                            pct_start=0.2,  # 20% warmup\n","                                            div_factor=3.0,\n","                                            final_div_factor=100)\n","    else:\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer,\n","            mode='min',\n","            factor=0.5,\n","            patience=5,  # Increase patience for large batch size\n","            min_lr=1e-6,\n","            threshold=1e-4\n","        )\n","\n","    # Select loss function based on version\n","    if LOSS_VERSION == 'v2':\n","        loss_fn = lambda y_pred, y_true: improved_weighted_focal_loss_v2(\n","            y_pred, y_true,\n","            base_pos_weight=BASE_POS_WEIGHT,\n","            fn_penalty=FN_PENALTY,\n","            fp_penalty=FP_PENALTY,\n","            is_prob_target=IS_PROB_TARGET,\n","            target_type=TARGET_TYPE,\n","            confidence_margin=CONFIDENCE_MARGIN,\n","            confidence_weight=CONFIDENCE_WEIGHT\n","        )\n","    else:  # 'v1' or default\n","        loss_fn = lambda y_pred, y_true: improved_weighted_focal_loss_v1(\n","            y_pred, y_true,\n","            base_pos_weight=BASE_POS_WEIGHT,\n","            fn_penalty=FN_PENALTY,\n","            fp_penalty=FP_PENALTY,\n","            is_prob_target=IS_PROB_TARGET\n","        )\n","\n","    # Training loop\n","    best_val_auc = 0\n","    losses = []\n","    val_metrics_history = []\n","\n","    print(\"\\nStarting training loop...\")\n","    for epoch in range(EPOCHS):\n","        epoch_start = time.time()\n","        model.train()\n","        total_loss = 0\n","\n","        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n","\n","        # In training loop\n","        running_fnr = []\n","        fnr_window = 100\n","\n","        try:\n","            for i, (X, y) in enumerate(pbar):\n","                # Move data to device at the start\n","                X = X.to(device)\n","                y = y.to(device)\n","\n","                # Clear gradients at start of forward pass\n","                optimizer.zero_grad()\n","\n","                # Forward pass\n","                logits = model(X)\n","\n","                # Add noise to logits\n","                logits = logits + torch.randn_like(logits) * 0.01\n","\n","                # Calculate loss\n","                loss = loss_fn(logits, y)  # y is already on device\n","\n","                # Backward pass\n","                loss.backward()\n","\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_NORM)\n","\n","                # Update weights and learning rate\n","                optimizer.step()\n","                if USE_WARMUP_SCHEDULER:\n","                    scheduler.step()  # OneCycleLR update\n","                total_loss += loss.item()\n","                current_loss = total_loss / (i + 1)\n","\n","                pbar.set_postfix({\n","                    'loss': f'{current_loss:.4f}',\n","                })\n","\n","                # Debug: Check predictions\n","                if i % LOG_INTERVAL == 0:\n","                    with torch.no_grad():\n","                        probs = torch.sigmoid(logits)\n","                        pos_preds = (probs > 0.5).sum().item()\n","                        avg_prob = probs.mean().item()\n","                        pos_labels = y.sum().item()\n","                        batch_fnr = ((y == 1) & (probs < 0.5)).float().mean()\n","\n","                        running_fnr.append(batch_fnr.item())\n","                        if len(running_fnr) > fnr_window:\n","                            running_fnr.pop(0)\n","\n","                        avg_fnr = sum(running_fnr) / len(running_fnr)\n","                        print(f\"Batch {i}: Pos Pred: {pos_preds}, Pos Labels: {pos_labels}, \"\n","                              f\"Avg Prob: {avg_prob:.4f}, Avg FNR = {avg_fnr:.4f}\")\n","\n","        except RuntimeError as e:\n","            if \"out of memory\" in str(e):\n","                print(\"WARNING: out of memory\")\n","                if torch.cuda.is_available():\n","                    torch.cuda.empty_cache()\n","                    gc.collect()\n","                continue\n","            else:\n","                raise e\n","\n","        # Record epoch loss\n","        epoch_loss = total_loss / len(train_loader)\n","        losses.append(epoch_loss)\n","\n","        # Evaluate\n","        val_metrics = evaluate(model, val_loader, device,\n","                              add_noise=EVAL_ADD_NOISE,\n","                              noise_std=EVAL_NOISE_STD,\n","                              is_prob_target=IS_PROB_TARGET)\n","        val_metrics_history.append(val_metrics)\n","\n","        # Log all metrics\n","        for metric_name, metric_value in val_metrics.items():\n","            writer.add_scalar(f'val/{metric_name}', metric_value, epoch)\n","\n","        # NOTE: only for ReduceLROnPlateau\n","        if not USE_WARMUP_SCHEDULER:\n","            scheduler.step(val_metrics['false_negative_rate'])  # Monitor FNR instead of AUC-PR\n","\n","        # Save best model with hyperparameters in name\n","        if val_metrics['auc_pr'] > best_val_auc:\n","            best_val_auc = val_metrics['auc_pr']\n","            model_save_path = os.path.join(LOG_DIR, f'best_{MODEL_NAME}.pt')\n","            torch.save({\n","                'model_state_dict': model.state_dict(),\n","                'hyperparameters': {\n","                    'n_layers': N_LAYERS,\n","                    'd_model': D_MODEL,\n","                    'n_heads': N_HEADS,\n","                    'focal_gamma': FOCAL_GAMMA,\n","                    'base_pos_weight': BASE_POS_WEIGHT,\n","                    'fn_penalty': FN_PENALTY,\n","                    'fp_penalty': FP_PENALTY,\n","                    'is_prob_target': IS_PROB_TARGET,\n","                    'target_col': TARGET_COL,\n","                    'dropout': DROPOUT,  # Add dropout to saved hyperparameters\n","                    'loss_version': LOSS_VERSION,\n","                    'target_type': TARGET_TYPE\n","                }\n","            }, model_save_path)\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= early_stopping_patience:\n","                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n","                break\n","\n","        print(f\"Epoch {epoch}: Loss = {epoch_loss:.4f}, \"\n","              f\"Val AUC-PR = {val_metrics['auc_pr']:.4f} (fixed: {val_metrics['fixed_auc_pr']:.4f}), \"\n","              f\"Val Accuracy = {val_metrics['accuracy']:.4f} (fixed: {val_metrics['fixed_accuracy']:.4f}), \"\n","              f\"Val Recall = {val_metrics['recall']:.4f} (fixed: {val_metrics['fixed_recall']:.4f}), \"\n","              f\"Val Precision = {val_metrics['precision']:.4f} (fixed: {val_metrics['fixed_precision']:.4f}), \"\n","              f\"Val F1 = {val_metrics['f1']:.4f} (fixed: {val_metrics['fixed_f1']:.4f}), \"\n","              f\"FNR = {val_metrics['false_negative_rate']:.4f} (fixed: {val_metrics['fixed_fnr']:.4f}), \"\n","              f\"FPR = {val_metrics['false_positive_rate']:.4f} (fixed: {val_metrics['fixed_fpr']:.4f}), \"\n","              f\"opt Threshold = {val_metrics['optimal_threshold']:.4f}, \"\n","              f\"TP = {val_metrics['true_positives']} (fixed: {val_metrics['fixed_true_positives']}), \"\n","              f\"TN = {val_metrics['true_negatives']} (fixed: {val_metrics['fixed_true_negatives']}), \"\n","              f\"FP = {val_metrics['false_positives']} (fixed: {val_metrics['fixed_false_positives']}), \"\n","              f\"FN = {val_metrics['false_negatives']} (fixed: {val_metrics['fixed_false_negatives']}), \"\n","              f\"Uncertain%: {val_metrics['uncertain_ratio']:.4f}, \"\n","              f\"Uncertain Acc: {val_metrics['uncertain_accuracy']:.4f}\")\n","\n","        # Add probability metrics only if they exist\n","        if 'mse' in val_metrics:\n","            print(f\"MSE = {val_metrics['mse']:.4f}, \"\n","                  f\"MAE = {val_metrics['mae']:.4f}, \"\n","                  f\"RMSE = {val_metrics['rmse']:.4f}, \"\n","                  f\"Prob Correlation = {val_metrics['prob_correlation']:.4f}, \"\n","                  f\"Mean Pred Prob = {val_metrics['mean_pred_prob']:.4f}, \"\n","                  f\"Mean True Prob = {val_metrics['mean_true_prob']:.4f}\")\n","\n","        epoch_time = time.time() - epoch_start\n","        print(f\"Total epoch time: {epoch_time:.2f}s\")\n","\n","    total_time = time.time() - start_time\n","    print(f\"\\nTotal training time: {total_time:.2f} seconds\")\n","\n","    # Plot training history\n","    plot_training_history(\n","        losses,\n","        val_metrics_history,\n","        os.path.join(LOG_DIR, 'training_history.png')\n","    )\n","\n","    if NEED_TEST_DATA:\n","        # Create test loader\n","        test_loader = create_multi_target_test_loader(\n","            TEST_DATA_PATH,\n","            target_col=TARGET_COL,  # Use the same target column as training\n","            batch_size=BATCH_SIZE\n","        )\n","\n","        # After training completes, evaluate on test set\n","        print(\"\\nEvaluating best model on test set...\")\n","        # Load best model\n","        checkpoint = torch.load(os.path.join(LOG_DIR, f'best_{MODEL_NAME}.pt'))\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","\n","        # Use existing evaluate function to get predictions and metrics\n","        test_metrics = evaluate(model, test_loader, device,\n","                              add_noise=EVAL_ADD_NOISE,\n","                              noise_std=EVAL_NOISE_STD,\n","                              is_prob_target=IS_PROB_TARGET)\n","\n","        # Print metrics in detailed format\n","        print(\"\\nTest Set Metrics:\")\n","        print(f\"AUC-PR = {test_metrics['auc_pr']:.4f} (fixed: {test_metrics['fixed_auc_pr']:.4f})\")\n","        print(f\"Accuracy = {test_metrics['accuracy']:.4f} (fixed: {test_metrics['fixed_accuracy']:.4f})\")\n","        print(f\"Recall = {test_metrics['recall']:.4f} (fixed: {test_metrics['fixed_recall']:.4f})\")\n","        print(f\"Precision = {test_metrics['precision']:.4f} (fixed: {test_metrics['fixed_precision']:.4f})\")\n","        print(f\"F1 = {test_metrics['f1']:.4f} (fixed: {test_metrics['fixed_f1']:.4f})\")\n","        print(f\"FNR = {test_metrics['false_negative_rate']:.4f} (fixed: {test_metrics['fixed_fnr']:.4f})\")\n","        print(f\"FPR = {test_metrics['false_positive_rate']:.4f} (fixed: {test_metrics['fixed_fpr']:.4f})\")\n","        print(f\"opt Threshold = {test_metrics['optimal_threshold']:.4f}\")\n","        print(f\"TP = {test_metrics['true_positives']} (fixed: {test_metrics['fixed_true_positives']})\")\n","        print(f\"TN = {test_metrics['true_negatives']} (fixed: {test_metrics['fixed_true_negatives']})\")\n","        print(f\"FP = {test_metrics['false_positives']} (fixed: {test_metrics['fixed_false_positives']})\")\n","        print(f\"FN = {test_metrics['false_negatives']} (fixed: {test_metrics['fixed_false_negatives']})\")\n","        print(f\"Uncertain%: {test_metrics['uncertain_ratio']:.4f}\")\n","        print(f\"Uncertain Acc: {test_metrics['uncertain_accuracy']:.4f}\")\n","\n","        # Print probability metrics if available\n","        if IS_PROB_TARGET and 'mse' in test_metrics:\n","            print(f\"\\nProbability Metrics:\")\n","            print(f\"MSE = {test_metrics['mse']:.4f}\")\n","            print(f\"MAE = {test_metrics['mae']:.4f}\")\n","            print(f\"RMSE = {test_metrics['rmse']:.4f}\")\n","            print(f\"Prob Correlation = {test_metrics['prob_correlation']:.4f}\")\n","            print(f\"Mean Pred Prob = {test_metrics['mean_pred_prob']:.4f}\")\n","            print(f\"Mean True Prob = {test_metrics['mean_true_prob']:.4f}\")\n","\n","        # Save predictions and true values\n","        if not os.path.exists(TEST_OUTPUT_DIR):\n","            os.makedirs(TEST_OUTPUT_DIR)\n","\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        output_path = os.path.join(TEST_OUTPUT_DIR, f\"predictions_{TARGET_COL}_{timestamp}.csv\")\n","\n","        # Get predictions and labels from test loader\n","        model.eval()\n","        all_preds = []\n","        all_labels = []\n","        with torch.no_grad():\n","            for X, y in test_loader:\n","                X = X.to(device)\n","                output = model(X)\n","                probs = torch.sigmoid(output)\n","                all_preds.extend(probs.cpu().numpy())\n","                all_labels.extend(y.cpu().numpy())\n","\n","        # Save predictions with target values\n","        results_df = pd.DataFrame({\n","            'predictions': all_preds,\n","            'targets': all_labels\n","        })\n","        results_df.to_csv(output_path, index=False)\n","        print(f\"\\nSaved predictions to {output_path}\")\n","\n","        # Log test metrics to tensorboard\n","        for metric_name, metric_value in test_metrics.items():\n","            writer.add_scalar(f'test/{metric_name}', metric_value, 0)\n","    else:\n","        print(\"Skipping test evaluation.\")\n","\n","    return model\n","\n","def train_traditional_baselines():\n","    \"\"\"Train and evaluate traditional baseline models (Logistic, XGBoost)\"\"\"\n","    start_time = time.time()\n","    writer = SummaryWriter(LOG_DIR)\n","\n","    # Early stopping parameters\n","    early_stopping_patience = 10\n","    early_stopping_counter = 0\n","    best_val_auc = 0\n","\n","    # Create dataloaders with validation data path\n","    print(\"\\nCreating dataloaders for traditional models...\")\n","    dataloader_start = time.time()\n","\n","    # Use multi-target dataloaders if TARGET_COL is defined, otherwise use regular dataloaders\n","    if 'TARGET_COL' in globals() and TARGET_COL:\n","        print(f\"Using multi-target dataloaders with target column: {TARGET_COL}\")\n","        train_loader, val_loader = create_multi_target_baseline_dataloaders(\n","            TRAIN_DATA_PATH,\n","            target_col=TARGET_COL,\n","            batch_size=BATCH_SIZE,\n","            train_ratio=TRAIN_RATIO,\n","            val_data_path=VAL_DATA_PATH\n","        )\n","        is_prob_target = '_prob' in TARGET_COL\n","    else:\n","        print(\"Using standard dataloaders\")\n","        train_loader, val_loader = create_baseline_dataloaders(\n","            TRAIN_DATA_PATH,\n","            batch_size=BATCH_SIZE,\n","            train_ratio=TRAIN_RATIO,\n","            val_data_path=VAL_DATA_PATH\n","        )\n","        is_prob_target = False\n","\n","    print(f\"Dataloader creation took {time.time() - dataloader_start:.2f} seconds\")\n","\n","    # Check class distribution\n","    train_labels = np.concatenate([y.numpy() for _, y in train_loader])\n","    val_labels = np.concatenate([y.numpy() for _, y in val_loader])\n","    print(f\"Training class distribution: {np.bincount(train_labels.astype(int))}\")\n","    print(f\"Validation class distribution: {np.bincount(val_labels.astype(int))}\")\n","\n","    # Get input size from the first batch\n","    sample_batch = next(iter(train_loader))\n","    input_size = sample_batch[0].shape[1]\n","    print(f\"Input feature size: {input_size}\")\n","\n","    # Initialize traditional models\n","    baselines = BaselineModels(\n","        pos_weight=BASE_POS_WEIGHT,\n","        fn_penalty=FN_PENALTY,\n","        fp_penalty=FP_PENALTY\n","    )\n","\n","    # Train and evaluate traditional models\n","    val_metrics_history = []\n","    models_info = [\n","        ('Logistic', 'logistic'),\n","        ('XGBoost', 'xgb')\n","    ]\n","\n","    for display_name, model_name in models_info:\n","        print(f\"\\nTraining {display_name}...\")\n","        model_start = time.time()\n","\n","        # Prepare training data\n","        X_train = np.concatenate([X.numpy() for X, _ in train_loader])\n","        y_train = np.concatenate([y.numpy() for _, y in train_loader])\n","\n","        # Get the appropriate fit function\n","        if model_name == 'logistic':\n","            fit_fn = lambda X, y: baselines.fit_logistic(X, y, is_prob_target=is_prob_target)\n","        else:\n","            fit_fn = lambda X, y: baselines.fit_xgb(X, y, is_prob_target=is_prob_target)\n","\n","        # Train model\n","        fit_fn(X_train, y_train)\n","        print(f\"{display_name} training took {time.time() - model_start:.2f} seconds\")\n","\n","        # Create prediction function\n","        predict_fn = lambda X: baselines.predict(model_name, X)\n","\n","        # Evaluate on validation set\n","        val_metrics = evaluate_baseline(predict_fn, val_loader,\n","                                     add_noise=EVAL_ADD_NOISE,\n","                                     noise_std=EVAL_NOISE_STD,\n","                                     is_prob_target=is_prob_target)\n","        val_metrics_history.append(val_metrics)\n","\n","        # Log metrics\n","        for metric_name, metric_value in val_metrics.items():\n","            writer.add_scalar(f'{model_name.lower()}/val/{metric_name}', metric_value, 0)\n","\n","        # Save model if it's the best so far\n","        if val_metrics['auc_pr'] > best_val_auc:\n","            best_val_auc = val_metrics['auc_pr']\n","            model_save_path = os.path.join(LOG_DIR, f'best_{model_name.lower()}_{MODEL_NAME}.pt')\n","            baselines.save_model(model_name, model_save_path)\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","\n","        # Print validation metrics in detail\n","        print(f\"\\n{display_name} Validation Metrics:\")\n","        print(f\"AUC-PR = {val_metrics['auc_pr']:.4f} (fixed: {val_metrics['fixed_auc_pr']:.4f})\")\n","        print(f\"Accuracy = {val_metrics['accuracy']:.4f} (fixed: {val_metrics['fixed_accuracy']:.4f})\")\n","        print(f\"Recall = {val_metrics['recall']:.4f} (fixed: {val_metrics['fixed_recall']:.4f})\")\n","        print(f\"Precision = {val_metrics['precision']:.4f} (fixed: {val_metrics['fixed_precision']:.4f})\")\n","        print(f\"F1 = {val_metrics['f1']:.4f} (fixed: {val_metrics['fixed_f1']:.4f})\")\n","        print(f\"FNR = {val_metrics['false_negative_rate']:.4f} (fixed: {val_metrics['fixed_fnr']:.4f})\")\n","        print(f\"FPR = {val_metrics['false_positive_rate']:.4f} (fixed: {val_metrics['fixed_fpr']:.4f})\")\n","        print(f\"opt Threshold = {val_metrics['optimal_threshold']:.4f}\")\n","        print(f\"TP = {val_metrics['true_positives']} (fixed: {val_metrics['fixed_true_positives']})\")\n","        print(f\"TN = {val_metrics['true_negatives']} (fixed: {val_metrics['fixed_true_negatives']})\")\n","        print(f\"FP = {val_metrics['false_positives']} (fixed: {val_metrics['fixed_false_positives']})\")\n","        print(f\"FN = {val_metrics['false_negatives']} (fixed: {val_metrics['fixed_false_negatives']})\")\n","        print(f\"Uncertain%: {val_metrics['uncertain_ratio']:.4f}\")\n","        print(f\"Uncertain Acc: {val_metrics['uncertain_accuracy']:.4f}\")\n","\n","    if NEED_TEST_DATA:\n","        # Create and evaluate on test set\n","        if 'TARGET_COL' in globals() and TARGET_COL:\n","            test_loader = create_multi_target_baseline_test_loader(\n","                TEST_DATA_PATH,\n","                target_col=TARGET_COL,\n","                batch_size=BATCH_SIZE\n","            )\n","        else:\n","            test_loader = create_baseline_test_loader(\n","                TEST_DATA_PATH,\n","                batch_size=BATCH_SIZE\n","            )\n","\n","        print(\"\\nEvaluating traditional models on test set...\")\n","        for display_name, model_name in models_info:\n","            predict_fn = lambda X: baselines.predict(model_name, X)\n","            test_metrics = evaluate_baseline(predict_fn, test_loader,\n","                                          add_noise=EVAL_ADD_NOISE,\n","                                          noise_std=EVAL_NOISE_STD,\n","                                          is_prob_target=is_prob_target)\n","\n","            # Log test metrics\n","            print(f\"\\n{display_name} Test Metrics:\")\n","            for metric_name, metric_value in test_metrics.items():\n","                writer.add_scalar(f'{model_name.lower()}/test/{metric_name}', metric_value, 0)\n","                print(f\"{metric_name}: {metric_value:.4f}\")\n","    else:\n","        print(\"Skipping test evaluation.\")\n","\n","    total_time = time.time() - start_time\n","    print(f\"\\nTotal traditional models training time: {total_time:.2f} seconds\")\n","\n","    return baselines\n","\n","def train_nn_baselines():\n","    \"\"\"Train and evaluate neural network baseline models (MLP, CNN)\"\"\"\n","    start_time = time.time()\n","    writer = SummaryWriter(LOG_DIR)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device} for neural network training\")\n","\n","    # Early stopping parameters\n","    early_stopping_patience = 10\n","    early_stopping_counter = 0\n","    best_val_auc = 0\n","\n","    # Create dataloaders with validation data path\n","    print(\"\\nCreating dataloaders for neural network models...\")\n","    dataloader_start = time.time()\n","\n","    # Use multi-target dataloaders if TARGET_COL is defined, otherwise use regular dataloaders\n","    if 'TARGET_COL' in globals() and TARGET_COL:\n","        print(f\"Using multi-target dataloaders with target column: {TARGET_COL}\")\n","        train_loader, val_loader = create_multi_target_baseline_dataloaders(\n","            TRAIN_DATA_PATH,\n","            target_col=TARGET_COL,\n","            batch_size=BATCH_SIZE,\n","            train_ratio=TRAIN_RATIO,\n","            val_data_path=VAL_DATA_PATH\n","        )\n","        is_prob_target = '_prob' in TARGET_COL\n","    else:\n","        print(\"Using standard dataloaders\")\n","        train_loader, val_loader = create_baseline_dataloaders(\n","            TRAIN_DATA_PATH,\n","            batch_size=BATCH_SIZE,\n","            train_ratio=TRAIN_RATIO,\n","            val_data_path=VAL_DATA_PATH\n","        )\n","        is_prob_target = False\n","\n","    print(f\"Dataloader creation took {time.time() - dataloader_start:.2f} seconds\")\n","\n","    # Check class distribution\n","    train_labels = np.concatenate([y.numpy() for _, y in train_loader])\n","    val_labels = np.concatenate([y.numpy() for _, y in val_loader])\n","    print(f\"Training class distribution: {np.bincount(train_labels.astype(int))}\")\n","    print(f\"Validation class distribution: {np.bincount(val_labels.astype(int))}\")\n","\n","    # Get input size from the first batch\n","    sample_batch = next(iter(train_loader))\n","    input_size = sample_batch[0].shape[1]\n","    print(f\"Input feature size: {input_size}\")\n","\n","    # Initialize neural network models\n","    nn_baselines = NNBaselineModels(\n","        input_size=input_size,\n","        pos_weight=BASE_POS_WEIGHT,\n","        fn_penalty=FN_PENALTY,\n","        fp_penalty=FP_PENALTY\n","    )\n","\n","    # Train and evaluate neural network models\n","    val_metrics_history = []\n","    nn_models_info = [\n","        ('MLP', 'mlp'),\n","        ('CNN', 'cnn')\n","    ]\n","\n","    for display_name, model_name in nn_models_info:\n","        print(f\"\\nTraining {display_name}...\")\n","        model_start = time.time()\n","\n","        # Train model using the dataloader directly\n","        nn_baselines.fit_model(\n","            model_name,\n","            train_loader,\n","            val_loader,\n","            epochs=NN_EPOCHS if 'NN_EPOCHS' in globals() else 10,\n","            lr=NN_LEARNING_RATE if 'NN_LEARNING_RATE' in globals() else 0.001\n","        )\n","        print(f\"{display_name} training took {time.time() - model_start:.2f} seconds\")\n","\n","        # Create prediction function\n","        predict_fn = lambda X: nn_baselines.predict(model_name, X)\n","\n","        # Evaluate on validation set\n","        val_metrics = evaluate_baseline(predict_fn, val_loader,\n","                                     add_noise=EVAL_ADD_NOISE,\n","                                     noise_std=EVAL_NOISE_STD,\n","                                     is_prob_target=is_prob_target)\n","        val_metrics_history.append(val_metrics)\n","\n","        # Log metrics\n","        for metric_name, metric_value in val_metrics.items():\n","            writer.add_scalar(f'{model_name.lower()}/val/{metric_name}', metric_value, 0)\n","\n","        # Save model if it's the best so far\n","        if val_metrics['auc_pr'] > best_val_auc:\n","            best_val_auc = val_metrics['auc_pr']\n","            model_save_path = os.path.join(LOG_DIR, f'best_{model_name.lower()}_{MODEL_NAME}.pt')\n","            nn_baselines.save_model(model_name, model_save_path)\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","\n","        # Print validation metrics in detail\n","        print(f\"\\n{display_name} Validation Metrics:\")\n","        print(f\"AUC-PR = {val_metrics['auc_pr']:.4f} (fixed: {val_metrics['fixed_auc_pr']:.4f})\")\n","        print(f\"Accuracy = {val_metrics['accuracy']:.4f} (fixed: {val_metrics['fixed_accuracy']:.4f})\")\n","        print(f\"Recall = {val_metrics['recall']:.4f} (fixed: {val_metrics['fixed_recall']:.4f})\")\n","        print(f\"Precision = {val_metrics['precision']:.4f} (fixed: {val_metrics['fixed_precision']:.4f})\")\n","        print(f\"F1 = {val_metrics['f1']:.4f} (fixed: {val_metrics['fixed_f1']:.4f})\")\n","        print(f\"FNR = {val_metrics['false_negative_rate']:.4f} (fixed: {val_metrics['fixed_fnr']:.4f})\")\n","        print(f\"FPR = {val_metrics['false_positive_rate']:.4f} (fixed: {val_metrics['fixed_fpr']:.4f})\")\n","        print(f\"opt Threshold = {val_metrics['optimal_threshold']:.4f}\")\n","        print(f\"TP = {val_metrics['true_positives']} (fixed: {val_metrics['fixed_true_positives']})\")\n","        print(f\"TN = {val_metrics['true_negatives']} (fixed: {val_metrics['fixed_true_negatives']})\")\n","        print(f\"FP = {val_metrics['false_positives']} (fixed: {val_metrics['fixed_false_positives']})\")\n","        print(f\"FN = {val_metrics['false_negatives']} (fixed: {val_metrics['fixed_false_negatives']})\")\n","        print(f\"Uncertain%: {val_metrics['uncertain_ratio']:.4f}\")\n","        print(f\"Uncertain Acc: {val_metrics['uncertain_accuracy']:.4f}\")\n","\n","    if NEED_TEST_DATA:\n","        # Create and evaluate on test set\n","        if 'TARGET_COL' in globals() and TARGET_COL:\n","            test_loader = create_multi_target_baseline_test_loader(\n","                TEST_DATA_PATH,\n","                target_col=TARGET_COL,\n","                batch_size=BATCH_SIZE\n","            )\n","        else:\n","            test_loader = create_baseline_test_loader(\n","                TEST_DATA_PATH,\n","                batch_size=BATCH_SIZE\n","            )\n","\n","        print(\"\\nEvaluating neural network models on test set...\")\n","        for display_name, model_name in nn_models_info:\n","            predict_fn = lambda X: nn_baselines.predict(model_name, X)\n","            test_metrics = evaluate_baseline(predict_fn, test_loader,\n","                                          add_noise=EVAL_ADD_NOISE,\n","                                          noise_std=EVAL_NOISE_STD,\n","                                          is_prob_target=is_prob_target)\n","\n","            # Log test metrics\n","            print(f\"\\n{display_name} Test Metrics:\")\n","            for metric_name, metric_value in test_metrics.items():\n","                writer.add_scalar(f'{model_name.lower()}/test/{metric_name}', metric_value, 0)\n","                print(f\"{metric_name}: {metric_value:.4f}\")\n","    else:\n","        print(\"Skipping test evaluation.\")\n","\n","    total_time = time.time() - start_time\n","    print(f\"\\nTotal neural network models training time: {total_time:.2f} seconds\")\n","\n","    return nn_baselines\n","\n","def evaluate_baseline(predict_fn, dataloader, add_noise=False, noise_std=1e-3, is_prob_target=False):\n","    \"\"\"Evaluate baseline model\"\"\"\n","    all_preds = []\n","    all_labels = []\n","\n","    for X, y in dataloader:\n","        # Handle both numpy arrays and tensors\n","        if isinstance(X, torch.Tensor):\n","            X_data = X  # Keep as tensor for NN models\n","            X_numpy = X.numpy()  # Convert to numpy for traditional models\n","        else:\n","            X_data = X\n","            X_numpy = X\n","\n","        if isinstance(y, torch.Tensor):\n","            y_numpy = y.numpy()\n","        else:\n","            y_numpy = y\n","\n","        # Try to use tensor input first (for NN models)\n","        try:\n","            pred = predict_fn(X_data)\n","        except:\n","            # Fall back to numpy input for traditional models\n","            pred = predict_fn(X_numpy)\n","\n","        # Convert predictions to numpy if they're tensors\n","        if isinstance(pred, torch.Tensor):\n","            pred = pred.cpu().numpy()\n","\n","        all_preds.extend(pred)\n","        all_labels.extend(y_numpy)\n","\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","\n","    # Log prediction distribution around 0.5\n","    exact_half = np.mean(all_preds == 0.5) * 100\n","    near_half_range = 0.01\n","    near_half = np.mean((all_preds > 0.5 - near_half_range) &\n","                       (all_preds < 0.5 + near_half_range)) * 100\n","\n","    print(f\"\\nPrediction distribution analysis:\")\n","    print(f\"Predictions exactly 0.5: {exact_half:.2f}%\")\n","    print(f\"Predictions within Â±{near_half_range} of 0.5: {near_half:.2f}%\")\n","\n","    # Optional noise addition\n","    if add_noise and exact_half > 0:\n","        print(f\"Adding noise (std={noise_std}) to break symmetry\")\n","        all_preds = all_preds + np.random.normal(0, noise_std, all_preds.shape)\n","\n","    return calculate_metrics(all_preds, all_labels, is_prob_target=is_prob_target)"]},{"cell_type":"code","source":["print(\"Starting training...\")\n","print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n","print(f\"Train Data path: {TRAIN_DATA_PATH}\")\n","print(f\"Val Data path: {VAL_DATA_PATH}\")\n","print(f\"Test Data path: {TEST_DATA_PATH}\")\n","print(f\"Log directory: {LOG_DIR}\")\n","print(f\"Target column: {TARGET_COL if 'TARGET_COL' in globals() else 'Default'}\")\n","\n","if model_type == 'transformer':\n","    print(\"\\n=== Training Transformer Model ===\")\n","    model = train_transformer()\n","elif model_type == 'traditional':\n","    print(\"\\n=== Training Traditional Baseline Models ===\")\n","    traditional_models = train_traditional_baselines()\n","elif model_type == 'nn':\n","    print(\"\\n=== Training Neural Network Baseline Models ===\")\n","    nn_models = train_nn_baselines()\n","elif model_type == 'all':\n","    print(\"\\n=== Training All Baseline Models ===\")\n","    traditional_models = train_traditional_baselines()\n","    nn_models = train_nn_baselines()\n","else:\n","    print(\"Invalid model type. Please choose 'transformer', 'traditional', 'nn', or 'all'.\")"],"metadata":{"id":"hPSOoHLhY4ya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["runtime.unassign()"],"metadata":{"id":"g-g1hdLoVBPZ"},"execution_count":null,"outputs":[]}]}