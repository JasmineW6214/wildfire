{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/your-repo/your-project/blob/main/v2/nb/generate_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"yG_zXwcelb87"},"source":["# Generate Sequential Dataset\n","\n","This notebook generates sequential datasets from processed wildfire data. It:\n","1. Loads processed data from parquet file\n","2. Creates sequences using sliding windows\n","3. Saves the sequences as a compressed npz file"]},{"cell_type":"code","metadata":{"id":"RCZd9Vuqlb88","executionInfo":{"status":"ok","timestamp":1740275996909,"user_tz":300,"elapsed":10,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"source":["# Install required packages\n","# !pip install pandas"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N1UuXEYdlb89","executionInfo":{"status":"ok","timestamp":1740276022185,"user_tz":300,"elapsed":25274,"user":{"displayName":"Ben","userId":"16626926592568551176"}},"outputId":"8997e6b4-575d-4d98-d2cd-0f7c9bfd5109"},"source":["# Mount Google Drive to access data\n","from google.colab import drive, runtime\n","import os\n","import time\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","import random\n","import json\n","import gc\n","\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["\"\"\"\n","NOTE: use hyperparams to name output dataset\n","\n","\"\"\"\n","# NOTE: test dataset\n","# OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/wildfire/data/test'\n","# YEARS = range(2023, 2024)  # 2021-2022\n","\n","# Update paths and parameters\n","BASE_INPUT_DIR = '/content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed_scaled'\n","# OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/wildfire/new_data/train_predict'\n","OUTPUT_DIR = '/content/drive/My Drive/Colab Notebooks/wildfire/new_data/test_predict'\n","# YEARS = [2022, 2021, 2020]  # 2020-2022\n","# YEARS = [2020, 2021, 2022, 2023]  # 2020-2022\n","# YEARS = [2019, 2020, 2021, 2022, 2023, 2024]  # 2020-2022\n","RESUME_FROM_YEAR = 2024\n","YEARS = [2025]  # 2025 for month 8-10 2024\n","\n","INPUT_DIRS = [os.path.join(BASE_INPUT_DIR, str(year)) for year in YEARS]\n","WINDOW_SIZE = 15 # 7, 30\n","PREDICTION_OFFSET = 15 # 7, 15, 30\n","NEG_POS_RATIO = 3 # 5\n","TEST_MODE = False\n","\n","LABEL = 'targetY'\n","LABELS = ['targetY', 'targetY_prob', 'targetY_o1', 'targetY_o1_prob',\n","          'targetY_o2', 'targetY_o2_prob', 'targetY_o3', 'targetY_o3_prob']"],"metadata":{"id":"hu7Yx8Cum-A6","executionInfo":{"status":"ok","timestamp":1740276022193,"user_tz":300,"elapsed":7,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"8BP_agEflb8-","executionInfo":{"status":"ok","timestamp":1740276022552,"user_tz":300,"elapsed":351,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"source":["\"\"\"\n","Generate Sequential Dataset\n","\n","This script generates sequential datasets from processed wildfire data. It:\n","1. Loads location mapping and processes each batch file\n","2. Creates sequences using sliding windows\n","3. Saves the sequences as compressed npz files by batch\n","\"\"\"\n","\n","\n","# Copy preprocessing code\n","# NUMERIC_FEATURES = [\n","#     'LST_Day_1km', 'LST_Night_1km', 'Emis_31', 'Emis_32',\n","#     'relative_humidity_2m_above_ground', 'u_component_of_wind_10m_above_ground',\n","#     'v_component_of_wind_10m_above_ground', 'precipitable_water_entire_atmosphere'\n","# ]\n","NUMERIC_FEATURES = ['LST_Day_1km', 'LST_Night_1km',\n","       'Emis_31', 'Emis_32', 'dewpoint_temperature_2m',\n","       'temperature_2m', 'soil_temperature_level_1',\n","       'surface_net_thermal_radiation', 'u_component_of_wind_10m',\n","       'v_component_of_wind_10m', 'surface_pressure', 'total_precipitation',\n","       'elevation', 'NDVI']\n","\n","GROUP_COLS = ['longitude', 'latitude']\n","\n","random.seed(42)\n","np.random.seed(42)\n","\n","def load_mapping(input_dir):\n","    \"\"\"Load location mapping from processed data directory.\"\"\"\n","    mapping_path = os.path.join(input_dir, 'location_mapping.parquet')\n","    mapping_df = pd.read_parquet(mapping_path)\n","    # Make sure the DataFrame has a MultiIndex\n","    if not isinstance(mapping_df.index, pd.MultiIndex):\n","        mapping_df.index = pd.MultiIndex.from_tuples(\n","            mapping_df.index,\n","            names=['longitude', 'latitude']\n","        )\n","    return mapping_df\n","\n","def generate_sequences_from_group(group_df, window_size, location_mapping_idx, prediction_offset=0, feature_cols=None, year=None):\n","    \"\"\"Generate sequences from a single location group.\"\"\"\n","    if feature_cols is None:\n","        feature_cols = NUMERIC_FEATURES\n","\n","    Xs_pos = []\n","    ys_pos = {label: [] for label in LABELS}\n","    seq_meta_pos = []\n","    Xs_neg = []\n","    ys_neg = {label: [] for label in LABELS}\n","    seq_meta_neg = []\n","\n","    group_df = group_df.sort_values('date').reset_index(drop=True)\n","    if len(group_df) < window_size + prediction_offset:\n","        return None, None, None, None, None, None\n","\n","    features = group_df[feature_cols].values\n","    targets = {label: group_df[label].values for label in LABELS}\n","\n","    # # Debug: Print positive cases in input data\n","    # for label in LABELS:\n","    #     if '_prob' in label:\n","    #         positive_mask = group_df[label.replace('_prob', '')] == 1\n","    #         if positive_mask.any():\n","    #             print(f\"\\nDebug - Input positive {label} values:\")\n","    #             print(f\"Values: {group_df[label][positive_mask].values}\")\n","\n","    for i in range(len(group_df) - window_size - prediction_offset + 1):\n","        seq = features[i:i+window_size]\n","\n","        # Get target values for all labels\n","        target_dict = {}\n","        if prediction_offset > 0:\n","            for label in LABELS:\n","                target_window = targets[label][i+window_size-1:i+window_size+prediction_offset-1]\n","                if '_prob' in label:\n","                    # For probability targets, take the maximum probability in the prediction window\n","                    target_dict[label] = np.max(target_window) if len(target_window) > 0 else 0\n","                else:\n","                    # For binary targets, check if any value in the prediction window is 1\n","                    target_dict[label] = 1 if np.any(target_window == 1) else 0\n","        else:\n","            # When prediction_offset is 0, use the immediate next value\n","            target_dict = {label: targets[label][i+window_size-1] for label in LABELS}\n","\n","        # Get the date range for this sequence\n","        start_date = group_df['date'].iloc[i]\n","        end_date = group_df['date'].iloc[i+window_size-1]\n","\n","        seq_metadata = {\n","            'location_mapping_idx': location_mapping_idx,\n","            'window_start': i,\n","            'window_size': window_size,\n","            'start_date': start_date.strftime('%Y-%m-%d') if isinstance(start_date, pd.Timestamp) else start_date,\n","            'end_date': end_date.strftime('%Y-%m-%d') if isinstance(end_date, pd.Timestamp) else end_date,\n","            'prediction_offset': prediction_offset,\n","            'year': year\n","        }\n","\n","        # Use primary target (targetY) for positive/negative split\n","        if target_dict['targetY'] == 1:\n","            # # Debug: Print probability values when adding a positive case\n","            # for label in LABELS:\n","            #     if '_prob' in label:\n","            #         print(f\"\\nDebug - Adding positive case:\")\n","            #         print(f\"Date: {end_date}\")\n","            #         print(f\"{label} value: {target_dict[label]}\")\n","\n","            Xs_pos.append(seq)\n","            for label in LABELS:\n","                ys_pos[label].append(target_dict[label])\n","            seq_meta_pos.append(seq_metadata)\n","        else:\n","            Xs_neg.append(seq)\n","            for label in LABELS:\n","                ys_neg[label].append(target_dict[label])\n","            seq_meta_neg.append(seq_metadata)\n","\n","    # Debug: Print summary of positive probability values\n","    # for label in LABELS:\n","    #     if '_prob' in label and ys_pos.get(label):\n","    #         print(f\"\\nDebug - Final {label} positive values summary:\")\n","    #         values = np.array(ys_pos[label])\n","    #         print(f\"Count: {len(values)}\")\n","    #         print(f\"Unique values: {np.unique(values)}\")\n","    #         print(f\"Min: {values.min():.4f}\")\n","    #         print(f\"Max: {values.max():.4f}\")\n","    #         print(f\"Mean: {values.mean():.4f}\")\n","\n","    return (np.array(Xs_pos) if Xs_pos else None,\n","            {k: np.array(v) for k, v in ys_pos.items()} if ys_pos[LABELS[0]] else None,\n","            seq_meta_pos if seq_meta_pos else None,\n","            np.array(Xs_neg) if Xs_neg else None,\n","            {k: np.array(v) for k, v in ys_neg.items()} if ys_neg[LABELS[0]] else None,\n","            seq_meta_neg if seq_meta_neg else None)\n","\n","def process_batch_file(batch_path, mapping_df, window_size, prediction_offset=0, neg_pos_ratio=3, year=None):\n","    \"\"\"Process a single batch file and generate sequences.\"\"\"\n","    try:\n","        batch_df = pd.read_parquet(batch_path)\n","        all_X_pos = []\n","        all_y_pos = {label: [] for label in LABELS}\n","        all_meta_pos = []\n","        all_X_neg = []\n","        all_y_neg = {label: [] for label in LABELS}\n","        all_meta_neg = []\n","\n","        total_groups = len(batch_df.groupby(GROUP_COLS))\n","        print(f\"Processing {total_groups} location groups in batch...\")\n","\n","        for group_idx, ((lon, lat), group) in enumerate(batch_df.groupby(GROUP_COLS), 1):\n","            if group_idx % 10000 == 0:\n","                print(f\"Processing group {group_idx}/{total_groups}...\")\n","                # Force garbage collection periodically\n","                gc.collect()\n","\n","            try:\n","                location_mapping_idx = mapping_df.loc[(lon, lat)]['mapping_idx']\n","            except KeyError:\n","                print(f\"Warning: Location ({lon}, {lat}) not found in mapping. Skipping...\")\n","                continue\n","\n","            result = generate_sequences_from_group(\n","                group,\n","                window_size=window_size,\n","                location_mapping_idx=location_mapping_idx,\n","                prediction_offset=prediction_offset,\n","                year=year\n","            )\n","\n","            X_pos, y_pos, meta_pos, X_neg, y_neg, meta_neg = result\n","\n","            # Clear group data immediately\n","            del group\n","\n","            if X_pos is not None and y_pos is not None:\n","                all_X_pos.append(X_pos)\n","                for label in LABELS:\n","                    if len(y_pos[label]) > 0:\n","                        all_y_pos[label].append(y_pos[label])\n","                all_meta_pos.extend(meta_pos)\n","\n","                # Clear positive data\n","                del X_pos, y_pos, meta_pos\n","\n","            if X_neg is not None and y_neg is not None:\n","                all_X_neg.append(X_neg)\n","                for label in LABELS:\n","                    if len(y_neg[label]) > 0:\n","                        all_y_neg[label].append(y_neg[label])\n","                all_meta_neg.extend(meta_neg)\n","\n","                # Clear negative data\n","                del X_neg, y_neg, meta_neg\n","\n","        # Clear batch_df as it's no longer needed\n","        del batch_df\n","        gc.collect()\n","\n","        if not all_X_pos:\n","            print(f\"No positive samples in batch {os.path.basename(batch_path)}, skipping...\")\n","            return None, None, None\n","\n","        # Combine positive cases\n","        X_pos_combined = np.concatenate(all_X_pos)\n","        y_pos_combined = {}\n","        for label in LABELS:\n","            if all_y_pos[label]:\n","                filtered_arrays = [arr for arr in all_y_pos[label] if arr.size > 0]\n","                if filtered_arrays:\n","                    dtype = np.float32 if '_prob' in label else np.int8\n","                    y_pos_combined[label] = np.concatenate(filtered_arrays).astype(dtype)\n","                else:\n","                    y_pos_combined[label] = np.array([], dtype=np.float32 if '_prob' in label else np.int8)\n","            else:\n","                y_pos_combined[label] = np.array([], dtype=np.float32 if '_prob' in label else np.int8)\n","\n","        # Clear individual arrays after concatenation\n","        del all_X_pos\n","        del all_y_pos\n","        gc.collect()\n","\n","        if len(y_pos_combined[LABELS[0]]) == 0:\n","            return None, None, None\n","\n","        all_X = [X_pos_combined]\n","        all_y = {k: [v] for k, v in y_pos_combined.items()}\n","        all_metadata = all_meta_pos\n","\n","        # Process negative samples\n","        if all_X_neg:\n","            try:\n","                X_neg_combined = np.concatenate(all_X_neg)\n","                y_neg_combined = {}\n","                for label in LABELS:\n","                    filtered_arrays = [arr for arr in all_y_neg[label] if arr.size > 0]\n","                    if filtered_arrays:\n","                        y_neg_combined[label] = np.concatenate(filtered_arrays)\n","                    else:\n","                        y_neg_combined[label] = np.array([])\n","\n","                # Clear individual negative arrays\n","                del all_X_neg\n","                del all_y_neg\n","                gc.collect()\n","\n","                n_neg_total = len(y_neg_combined[LABELS[0]])\n","                n_neg_keep = min(n_neg_total, int(len(y_pos_combined[LABELS[0]]) * neg_pos_ratio))\n","\n","                if n_neg_keep > 0:\n","                    neg_indices = np.random.choice(n_neg_total, n_neg_keep, replace=False)\n","                    X_neg_combined = X_neg_combined[neg_indices]\n","                    for label in LABELS:\n","                        y_neg_combined[label] = y_neg_combined[label][neg_indices]\n","                    neg_metadata = [all_meta_neg[i] for i in neg_indices]\n","\n","                    all_X.append(X_neg_combined)\n","                    for label in LABELS:\n","                        all_y[label].append(y_neg_combined[label])\n","                    all_metadata.extend(neg_metadata)\n","\n","                    # Clear negative combined data\n","                    del X_neg_combined, y_neg_combined, neg_metadata\n","            except Exception as e:\n","                print(f\"Error processing negative samples: {str(e)}\")\n","\n","        # Clear remaining negative data\n","        del all_meta_neg\n","        gc.collect()\n","\n","        # Final combination\n","        X = np.concatenate(all_X)\n","        y = {k: np.concatenate(v) for k, v in all_y.items()}\n","\n","        # Clear intermediate arrays\n","        del all_X, all_y\n","        gc.collect()\n","\n","        # Shuffle\n","        shuffle_idx = np.random.permutation(len(y[LABELS[0]]))\n","        X = X[shuffle_idx]\n","        for label in LABELS:\n","            y[label] = y[label][shuffle_idx]\n","        metadata = [all_metadata[i] for i in shuffle_idx]\n","\n","        # Add sequence indices\n","        for idx, meta in enumerate(metadata):\n","            meta['sequence_idx'] = idx\n","\n","        print(f\"Final batch dataset composition: {np.sum(y[LABELS[0]])} positive, {len(y[LABELS[0]]) - np.sum(y[LABELS[0]])} negative samples\")\n","        return X, y, metadata\n","\n","    except Exception as e:\n","        print(f\"Error processing batch file: {str(e)}\")\n","        return None, None, None\n","    finally:\n","        # Final cleanup\n","        gc.collect()\n","\n","def trace_failed_prediction(sequence_idx, metadata_path, processed_data_base_dir):\n","    \"\"\"Trace a sequence back to its source time series data.\"\"\"\n","\n","    # Load sequence metadata\n","    with open(metadata_path, 'r') as f:\n","        sequence_metadata = json.load(f)\n","\n","    # Get specific sequence metadata\n","    seq_meta = next(meta for meta in sequence_metadata\n","                   if meta['sequence_idx'] == sequence_idx)\n","\n","    # Load location mapping for the year\n","    year = seq_meta['year']\n","    year_dir = os.path.join(processed_data_base_dir, str(year))\n","    mapping_df = pd.read_parquet(os.path.join(year_dir, 'location_mapping.parquet'))\n","\n","    # Find location information using mapping_idx\n","    location_info = mapping_df[mapping_df['mapping_idx'] == seq_meta['location_mapping_idx']].iloc[0]\n","    longitude, latitude = location_info.name\n","    batch_file = location_info['batch_file']\n","\n","    # Load and filter batch data\n","    batch_df = pd.read_parquet(os.path.join(year_dir, batch_file))\n","    location_data = batch_df[\n","        (batch_df['longitude'] == longitude) &\n","        (batch_df['latitude'] == latitude)\n","    ].sort_values('date')\n","\n","    # Extract the specific window\n","    window_data = location_data.iloc[\n","        seq_meta['window_start']:\n","        seq_meta['window_start'] + seq_meta['window_size']\n","    ]\n","\n","    return window_data\n","\n","def generate_and_save_sequences(input_dirs, output_dir, window_size, prediction_offset=0,\n","                              neg_pos_ratio=3, test_mode=False, resume_from_year=None):\n","    \"\"\"Generate sequences from all years and save them as a single dataset.\"\"\"\n","    start_time = time.time()\n","    print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting sequence generation\")\n","    print(f\"Parameters: window_size={window_size}, prediction_offset={prediction_offset}, \"\n","          f\"neg_pos_ratio={neg_pos_ratio}, test_mode={test_mode}\")\n","    if resume_from_year:\n","        print(f\"Resuming processing from year {resume_from_year}\")\n","\n","    os.makedirs(output_dir, exist_ok=True)\n","    temp_dir = os.path.join(output_dir, 'temp')\n","    os.makedirs(temp_dir, exist_ok=True)\n","\n","    # Create filename with hyperparameters\n","    years_str = f\"{min(YEARS)}-{max(YEARS)}\"\n","    base_filename = f\"sequences_y{years_str}_w{window_size}_o{prediction_offset}_r{neg_pos_ratio}\"\n","    if test_mode:\n","        base_filename += \"_test\"\n","\n","    year_files = []  # Store paths to temporary files\n","    total_sequences = 0\n","    total_locations = 0\n","    total_positive = 0\n","\n","    # First, collect existing temp files if resuming\n","    if resume_from_year:\n","        for year in YEARS:\n","            if year < resume_from_year:\n","                year_filename = f\"temp_{year}\"\n","                year_path = os.path.join(temp_dir, year_filename)\n","                if os.path.exists(f\"{year_path}.npz\"):\n","                    # Load metadata to get sequence count\n","                    with open(f\"{year_path}_metadata.json\", 'r') as f:\n","                        year_metadata = json.load(f)\n","                    year_files.append((year_path, len(year_metadata)))\n","                    total_sequences += len(year_metadata)\n","                    total_positive += sum(1 for meta in year_metadata if meta.get('is_positive', False))\n","                    print(f\"Found existing processed data for year {year}\")\n","\n","    # Process each year's data separately\n","    for input_dir in input_dirs:\n","        year = int(os.path.basename(input_dir))\n","\n","        # Skip years before resume_from_year\n","        if resume_from_year and year < resume_from_year:\n","            continue\n","\n","        print(f\"\\nProcessing year: {year}\")\n","\n","        year_X = []\n","        year_y = {k: [] for k in LABELS}\n","        year_metadata = []\n","\n","        # Load mapping for this year\n","        mapping_df = load_mapping(input_dir)\n","        batch_groups = list(mapping_df.groupby('batch_file'))\n","\n","        if test_mode:\n","            test_batch_count = 1  # Process last 2 batches in test mode\n","            original_count = len(batch_groups)\n","            batch_groups = batch_groups[:test_batch_count]\n","            print(f\"\\n[TEST MODE] Processing {len(batch_groups)} out of {original_count} batch files\")\n","\n","        for batch_idx, (batch_file, locations) in enumerate(batch_groups, 1):\n","            print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Processing {year} batch {batch_idx}/{len(batch_groups)}\")\n","\n","            # Process batch file\n","            batch_path = os.path.join(input_dir, batch_file)\n","            try:\n","                X, y, metadata = process_batch_file(\n","                    batch_path,\n","                    mapping_df,\n","                    window_size=window_size,\n","                    prediction_offset=prediction_offset,\n","                    neg_pos_ratio=neg_pos_ratio,\n","                    year=year\n","                )\n","\n","                if X is not None and y is not None and metadata is not None:\n","                    # Convert numpy types to Python native types in metadata\n","                    for meta in metadata:\n","                        meta['sequence_idx'] = int(meta['sequence_idx'])\n","                        meta['location_mapping_idx'] = int(meta['location_mapping_idx'])\n","                        meta['window_start'] = int(meta['window_start'])\n","                        meta['window_size'] = int(meta['window_size'])\n","                        meta['prediction_offset'] = int(meta['prediction_offset'])\n","                        if 'year' in meta:\n","                            meta['year'] = int(meta['year'])\n","                        # Convert dates to string format if they're not already\n","                        if isinstance(meta.get('start_date'), pd.Timestamp):\n","                            meta['start_date'] = meta['start_date'].strftime('%Y-%m-%d')\n","                        if isinstance(meta.get('end_date'), pd.Timestamp):\n","                            meta['end_date'] = meta['end_date'].strftime('%Y-%m-%d')\n","\n","                    year_X.append(X)\n","                    for label in LABELS:\n","                        year_y[label].append(y[label])\n","                    year_metadata.extend(metadata)\n","                    total_locations += len(locations)\n","\n","            finally:\n","                gc.collect()\n","\n","        # Combine and save year's data\n","        if year_X:  # Only save if we have data for this year\n","            year_X_combined = np.concatenate(year_X)\n","            year_y_combined = {k: np.concatenate(v) for k, v in year_y.items()}\n","\n","            # Shuffle year's data\n","            year_shuffle_idx = np.random.permutation(len(year_y_combined[LABELS[0]]))\n","            year_X_combined = year_X_combined[year_shuffle_idx]\n","            for label in LABELS:\n","                year_y_combined[label] = year_y_combined[label][year_shuffle_idx]\n","            year_metadata = [year_metadata[i] for i in year_shuffle_idx]\n","\n","            # Update sequence indices for the year\n","            for idx, meta in enumerate(year_metadata):\n","                meta['sequence_idx'] = idx\n","\n","            # Save year's data to temporary files\n","            year_filename = f\"temp_{year}\"\n","            year_path = os.path.join(temp_dir, year_filename)\n","            np.savez_compressed(f\"{year_path}.npz\", X=year_X_combined, **year_y_combined)\n","\n","            with open(f\"{year_path}_metadata.json\", 'w') as f:\n","                json.dump(year_metadata, f)\n","\n","            year_files.append((year_path, len(year_y_combined[LABELS[0]])))\n","            total_sequences += len(year_y_combined[LABELS[0]])\n","            total_positive += np.sum(year_y_combined[LABELS[0]])\n","\n","            print(f\"\\nYear {year} stats:\")\n","            print(f\"  Sequences: {len(year_y_combined[LABELS[0]])}\")\n","            print(f\"  Positive samples: {np.sum(year_y_combined[LABELS[0]])}\")\n","\n","            # Clear memory\n","            del year_X_combined, year_y_combined, year_metadata\n","            gc.collect()\n","\n","    # Combine all years' data\n","    if year_files:\n","        print(\"\\nCombining all years' data...\")\n","\n","        # Calculate indices for each year in final dataset\n","        current_idx = 0\n","        final_metadata = []\n","\n","        # First pass: load and update metadata\n","        for year_path, year_size in year_files:\n","            with open(f\"{year_path}_metadata.json\", 'r') as f:\n","                year_metadata = json.load(f)\n","\n","                # Update sequence indices\n","                for meta in year_metadata:\n","                    meta['sequence_idx'] += current_idx\n","\n","                final_metadata.extend(year_metadata)\n","                current_idx += year_size\n","\n","        # Second pass: combine data\n","        final_X = np.zeros((total_sequences, WINDOW_SIZE, len(NUMERIC_FEATURES)), dtype=np.float32)\n","        final_y_dict = {}\n","        for label in LABELS:\n","            # Use float32 for probability targets, int8 for binary targets\n","            dtype = np.float32 if '_prob' in label else np.int8\n","            final_y_dict[label] = np.zeros(total_sequences, dtype=dtype)\n","\n","        current_idx = 0\n","        for year_path, year_size in year_files:\n","            year_data = np.load(f\"{year_path}.npz\")\n","            final_X[current_idx:current_idx + year_size] = year_data['X']\n","            for label in LABELS:\n","                final_y_dict[label][current_idx:current_idx + year_size] = year_data[label]\n","            current_idx += year_size\n","\n","            # Clear memory after each year\n","            del year_data\n","            gc.collect()\n","\n","        # Final shuffle of complete dataset\n","        final_shuffle_idx = np.random.permutation(len(final_y_dict[LABELS[0]]))\n","        final_X = final_X[final_shuffle_idx]\n","        for label in LABELS:\n","            final_y_dict[label] = final_y_dict[label][final_shuffle_idx]\n","        final_metadata = [final_metadata[i] for i in final_shuffle_idx]\n","\n","        # Update sequence indices after final shuffle\n","        for idx, meta in enumerate(final_metadata):\n","            meta['sequence_idx'] = idx\n","\n","        # # Add validation\n","        # for label in LABELS:\n","        #     if label not in final_y_dict:\n","        #         raise ValueError(f\"Missing target column {label} in final dataset\")\n","\n","        # Save final combined dataset with all labels\n","        save_dict = {'X': final_X}\n","        for label in LABELS:\n","            save_dict[label] = final_y_dict[label]\n","        np.savez_compressed(os.path.join(output_dir, f'{base_filename}.npz'), **save_dict)\n","        with open(os.path.join(output_dir, f'{base_filename}_metadata.json'), 'w') as f:\n","            json.dump(final_metadata, f)\n","\n","        # Clean up temporary files\n","        # NOTE: disable for now\n","        # for year_path, _ in year_files:\n","        #     os.remove(f\"{year_path}.npz\")\n","        #     os.remove(f\"{year_path}_metadata.json\")\n","        # os.rmdir(temp_dir)\n","\n","        total_duration = time.time() - start_time\n","        print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Sequence generation summary:\")\n","        print(f\"Total time: {total_duration:.2f} seconds\")\n","        print(f\"Total sequences: {total_sequences}\")\n","        print(f\"Total locations: {total_locations}\")\n","        print(f\"Total positive samples: {total_positive} ({(total_positive/total_sequences)*100:.2f}%)\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHhcXbfnlb8_","outputId":"ccd20e2f-0016-4642-dc1c-6cab286b0a98","executionInfo":{"status":"ok","timestamp":1740280279597,"user_tz":300,"elapsed":4257042,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"source":["# Generate and save sequences with resume functionality\n","generate_and_save_sequences(\n","    input_dirs=INPUT_DIRS,\n","    output_dir=OUTPUT_DIR,\n","    window_size=WINDOW_SIZE,\n","    prediction_offset=PREDICTION_OFFSET,\n","    neg_pos_ratio=NEG_POS_RATIO,\n","    test_mode=TEST_MODE,\n","    resume_from_year=RESUME_FROM_YEAR  # Add this parameter to resume from 2023\n",")"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[02:00:30] Starting sequence generation\n","Parameters: window_size=15, prediction_offset=15, neg_pos_ratio=3, test_mode=False\n","Resuming processing from year 2024\n","\n","Processing year: 2025\n","\n","[02:00:37] Processing 2025 batch 1/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 913 positive, 2739 negative samples\n","\n","[02:05:09] Processing 2025 batch 2/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 748 positive, 2244 negative samples\n","\n","[02:09:08] Processing 2025 batch 3/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","No positive samples in batch batch_0003.parquet, skipping...\n","\n","[02:13:37] Processing 2025 batch 4/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 4524 positive, 13572 negative samples\n","\n","[02:18:19] Processing 2025 batch 5/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 3648 positive, 10944 negative samples\n","\n","[02:23:03] Processing 2025 batch 6/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","No positive samples in batch batch_0006.parquet, skipping...\n","\n","[02:27:43] Processing 2025 batch 7/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 750 positive, 2250 negative samples\n","\n","[02:32:23] Processing 2025 batch 8/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 570 positive, 1710 negative samples\n","\n","[02:37:05] Processing 2025 batch 9/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 1477 positive, 4431 negative samples\n","\n","[02:41:47] Processing 2025 batch 10/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 1500 positive, 4500 negative samples\n","\n","[02:46:29] Processing 2025 batch 11/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 1793 positive, 5379 negative samples\n","\n","[02:51:10] Processing 2025 batch 12/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 7805 positive, 23415 negative samples\n","\n","[02:55:52] Processing 2025 batch 13/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 11538 positive, 34614 negative samples\n","\n","[03:00:34] Processing 2025 batch 14/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 12196 positive, 36588 negative samples\n","\n","[03:05:16] Processing 2025 batch 15/16\n","Processing 50000 location groups in batch...\n","Processing group 10000/50000...\n","Processing group 20000/50000...\n","Processing group 30000/50000...\n","Processing group 40000/50000...\n","Processing group 50000/50000...\n","Final batch dataset composition: 1406 positive, 4218 negative samples\n","\n","[03:09:23] Processing 2025 batch 16/16\n","Processing 17323 location groups in batch...\n","Processing group 10000/17323...\n","Final batch dataset composition: 955 positive, 2865 negative samples\n","\n","Year 2025 stats:\n","  Sequences: 199292\n","  Positive samples: 49823\n","\n","Combining all years' data...\n","\n","[03:11:27] Sequence generation summary:\n","Total time: 4257.03 seconds\n","Total sequences: 199292\n","Total locations: 667323\n","Total positive samples: 49823 (25.00%)\n"]}]},{"cell_type":"code","source":["runtime.unassign()"],"metadata":{"id":"1sSBvjGTU_yy","executionInfo":{"status":"ok","timestamp":1740280279952,"user_tz":300,"elapsed":345,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":6,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}