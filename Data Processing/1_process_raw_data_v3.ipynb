{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/your-repo/your-project/blob/main/v2/nb/process_raw_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"OGLrwALRzC9e"},"source":["# Process Raw Wildfire Data\n","\n","This notebook processes raw parquet files containing wildfire data. It:\n","1. Loads raw data from parquet files\n","2. Processes each location's time series (interpolating missing values)\n","3. Saves the processed data for later use"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rN9KCe0pzC9f","executionInfo":{"status":"ok","timestamp":1739981363748,"user_tz":300,"elapsed":4139,"user":{"displayName":"Ben","userId":"16626926592568551176"}},"outputId":"80fdf579-9eda-40f3-880e-27262181790c"},"source":["# Install required packages\n","!pip install dask[dataframe] pyarrow"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (17.0.0)\n","Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.11/dist-packages (2024.10.0)\n","Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.1.8)\n","Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (3.1.1)\n","Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2024.10.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (24.2)\n","Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (1.4.2)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (6.0.2)\n","Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (0.12.1)\n","Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (8.6.1)\n","Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.11/dist-packages (from dask[dataframe]) (2.2.2)\n","Collecting dask-expr<1.2,>=1.1 (from dask[dataframe])\n","  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from pyarrow) (1.26.4)\n","INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n","  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n","  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=4.13.0->dask[dataframe]) (3.21.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[dataframe]) (2025.1)\n","Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask[dataframe]) (1.0.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[dataframe]) (1.17.0)\n","Downloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dask-expr\n","Successfully installed dask-expr-1.1.16\n"]}]},{"cell_type":"code","metadata":{"id":"M25gQ5m7zC9h","executionInfo":{"status":"ok","timestamp":1739981366997,"user_tz":300,"elapsed":3246,"user":{"displayName":"Ben","userId":"16626926592568551176"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"37d50033-7dd8-4455-832f-4f8b94c61978"},"source":["# Mount Google Drive to access data\n","from google.colab import drive, runtime\n","# Import required modules\n","import os\n","import glob\n","import time\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","\n","# Try to use Dask for efficient reading\n","try:\n","    import dask.dataframe as dd\n","    USE_DASK = True\n","except ImportError:\n","    USE_DASK = False\n","\n","print(f\"using dask: {USE_DASK}\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["using dask: True\n"]}]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"f6_JNbBrx48E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739981383153,"user_tz":300,"elapsed":16155,"user":{"displayName":"Ben","userId":"16626926592568551176"}},"outputId":"82b570e3-73d7-4337-ce29-db151821b65b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["year=2024\n","LOC_BATCH_SIZE=50000\n","\n","# Set paths\n","# DATA_DIR = '/content/drive/My Drive/Colab Notebooks/wildfire/data/raw/2023'  # Update this\n","# OUTPUT_PATH = '/content/drive/My Drive/Colab Notebooks/wildfire/data/processed/2023'  # Update this\n","# DATA_DIR = f'/content/drive/MyDrive/GEE_Exports/Alberta/{year}'  # Update this\n","DATA_DIR = f'/content/drive/MyDrive/GEE_Exports/Alberta/{year}/2024_8-10'  # Update this\n","# OUTPUT_PATH = f'/content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed/{year}'  # Update this\n","OUTPUT_PATH = f'/content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed/2025'  # Update this\n","OUTPUT_TOP_PATH = f'/content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed/'\n","\n","PARQUET_PATTERN=\"*_n5.parquet\"\n","\n","# DATA_DIR = f\"/content/drive/MyDrive/GEE_Exports/Alberta/{year}/final_files/final_file-{year}-{month}_n3.parquet\"\n","# OUTPUT_PATH = f'/content/drive/MyDrive/Colab Notebooks/wildfire/data/processed/{year}'\n","\n","NUMERIC_FEATURES = ['LST_Day_1km', 'LST_Night_1km',\n","       'Emis_31', 'Emis_32', 'dewpoint_temperature_2m',\n","       'temperature_2m', 'soil_temperature_level_1',\n","       'surface_net_thermal_radiation', 'u_component_of_wind_10m',\n","       'v_component_of_wind_10m', 'surface_pressure', 'total_precipitation', 'elevation', 'NDVI']\n","LABEL = 'targetY'\n","GROUP_COLS = ['longitude', 'latitude']\n","\n","# NEEDED_COLUMNS=['date', 'longitude', 'latitude', 'LST_Day_1km', 'LST_Night_1km',\n","#        'Emis_31', 'Emis_32', 'dewpoint_temperature_2m',\n","#        'temperature_2m', 'soil_temperature_level_1',\n","#        'surface_net_thermal_radiation', 'u_component_of_wind_10m',\n","#        'v_component_of_wind_10m', 'surface_pressure', 'total_precipitation','elevation', 'NDVI', 'targetY']\n","NEEDED_COLUMNS=['date', 'longitude', 'latitude', 'LST_Day_1km', 'LST_Night_1km',\n","       'Emis_31', 'Emis_32', 'dewpoint_temperature_2m',\n","       'temperature_2m', 'soil_temperature_level_1',\n","       'surface_net_thermal_radiation', 'u_component_of_wind_10m',\n","       'v_component_of_wind_10m', 'surface_pressure', 'total_precipitation','elevation', 'NDVI', 'targetY', 'targetY_o1', 'targetY_o2', 'targetY_o3', 'targetY_prob', 'targetY_o1_prob', 'targetY_o2_prob', 'targetY_o3_prob']"],"metadata":{"id":"o_DxNsLS_u3g","executionInfo":{"status":"ok","timestamp":1739981383172,"user_tz":300,"elapsed":3,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def validate_processed_batch(batch_df):\n","    \"\"\"Validate that a processed batch has no duplicates.\"\"\"\n","    total_records = len(batch_df)\n","    unique_records = len(batch_df.drop_duplicates(subset=['date', 'longitude', 'latitude']))\n","\n","    if total_records != unique_records:\n","        print(f\"WARNING: Found {total_records - unique_records} duplicates in processed batch!\")\n","        return False\n","\n","    print(\"Validation passed: No duplicates found in processed batch\")\n","    return True\n","\n","def process_single_group(df):\n","    \"\"\"Process a single location time series.\"\"\"\n","    # start_time = time.time()\n","\n","    # Sort by date\n","    df = df.sort_values('date').reset_index(drop=True)\n","\n","    # drop duplicates based on (data, longitude, latitude)\n","    # NOTE: not required, due to drop duplicates at previous feature combination step\n","    # df = df.drop_duplicates(subset=['date', 'longitude', 'latitude'])\n","\n","    # Interpolate missing values\n","    for col in NUMERIC_FEATURES:\n","        if col in df.columns and df[col].isnull().sum() > 0:\n","            # First try linear interpolation\n","            df[col] = df[col].interpolate(method='linear', limit_direction='both')\n","            # Then forward fill and backward fill any remaining nulls\n","            df[col] = df[col].ffill().bfill()\n","\n","    return df\n","\n","def extract_unique_locations(data_dir, file_pattern=PARQUET_PATTERN, output_dir=OUTPUT_TOP_PATH):\n","    \"\"\"Extract unique (longitude, latitude) pairs from one monthly file or load from cache.\"\"\"\n","    locations_cache_path = os.path.join(output_dir, 'all_locations_groups.csv')\n","\n","    # Check if cached locations file exists\n","    if os.path.exists(locations_cache_path):\n","        print(f\"\\nLoading cached locations from {locations_cache_path}\")\n","        unique_locations = pd.read_csv(locations_cache_path)\n","        print(f\"Loaded {len(unique_locations)} unique locations from cache\")\n","        return unique_locations\n","\n","    print(f\"\\nExtracting unique locations at {datetime.now().strftime('%H:%M:%S')}\")\n","\n","    # Get first parquet file only\n","    parquet_files = glob.glob(os.path.join(data_dir, file_pattern))\n","    first_file = parquet_files[0]\n","\n","    # Read only location columns\n","    location_df = pd.read_parquet(first_file, columns=['longitude', 'latitude'])\n","    unique_locations = location_df.drop_duplicates(subset=['longitude', 'latitude'])\n","\n","    # Cache the results\n","    print(f\"Caching {len(unique_locations)} unique locations to {locations_cache_path}\")\n","    unique_locations.to_csv(locations_cache_path, index=False)\n","\n","    return unique_locations\n","\n","def process_location_batch(location_batch, data_dir, file_pattern=PARQUET_PATTERN):\n","    \"\"\"Process a batch of locations across all monthly files.\"\"\"\n","    print(f\"Processing batch of {len(location_batch)} locations\")\n","\n","    # Get all monthly files\n","    parquet_files = sorted(glob.glob(os.path.join(data_dir, file_pattern)))\n","\n","    # Initialize empty list to store monthly data for this batch\n","    all_data = []\n","\n","    # Process each monthly file\n","    for file_path in parquet_files:\n","        print(f\"Reading file: {os.path.basename(file_path)}\")\n","\n","        # Read only the rows for our location batch\n","        df = pd.read_parquet(file_path, columns=NEEDED_COLUMNS)\n","        mask = df.set_index(['longitude', 'latitude']).index.isin(\n","            location_batch.set_index(['longitude', 'latitude']).index\n","        )\n","        batch_monthly = df[mask]\n","        \"\"\" NOTE: this is done in above preprocessing step\n","        # Drop any duplicates within this file\n","        initial_count = len(batch_monthly)\n","        batch_monthly = batch_monthly.drop_duplicates(\n","            subset=['date', 'longitude', 'latitude'],\n","            keep='last'  # Keep the last occurrence in case of duplicates\n","        )\n","        if initial_count != len(batch_monthly):\n","            print(f\"  Removed {initial_count - len(batch_monthly)} duplicates from {os.path.basename(file_path)}\")\n","        \"\"\"\n","        all_data.append(batch_monthly)\n","        del df, batch_monthly\n","\n","    # Combine all monthly data for this batch\n","    print(\"Combining data from all files...\")\n","    combined_batch = pd.concat(all_data, ignore_index=True)\n","    initial_count = len(combined_batch)\n","\n","    # Remove duplicates across files, keeping the latest version of each record\n","    combined_batch = combined_batch.sort_values('date').drop_duplicates(\n","        subset=['date', 'longitude', 'latitude'],\n","        keep='last'\n","    )\n","\n","    if initial_count != len(combined_batch):\n","        print(f\"Removed {initial_count - len(combined_batch)} duplicate records across files\")\n","\n","    del all_data\n","    return combined_batch\n","\n","def batch_process_locations(data_dir, output_dir, batch_size=1000):\n","    \"\"\"Process locations in batches to manage memory usage.\"\"\"\n","    batch_start = time.time()\n","    print(f\"\\n{'='*80}\")\n","    print(f\"Starting batch processing at {datetime.now().strftime('%H:%M:%S')}\")\n","    print(f\"Data directory: {data_dir}\")\n","    print(f\"Output directory: {output_dir}\")\n","    print(f\"Batch size: {batch_size}\")\n","    print(f\"{'='*80}\\n\")\n","\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # First, get all unique locations from one file\n","    unique_locations = extract_unique_locations(data_dir)\n","    total_locations = len(unique_locations)\n","\n","    # Process locations in batches\n","    total_batches = (total_locations + batch_size - 1) // batch_size\n","    location_mapping = {}\n","\n","    print(f\"\\nProcessing {total_locations} locations in {total_batches} batches\")\n","    print(f\"{'='*80}\")\n","\n","    for batch_idx in range(total_batches):\n","    # for batch_idx in range(1):\n","        batch_iteration_start = time.time()\n","        start_idx = batch_idx * batch_size\n","        end_idx = min((batch_idx + 1) * batch_size, total_locations)\n","\n","        print(f\"\\nBatch {batch_idx + 1}/{total_batches}\")\n","        print(f\"Processing locations {start_idx} to {end_idx} ({end_idx - start_idx} locations)\")\n","\n","        # Get location batch\n","        location_batch = unique_locations.iloc[start_idx:end_idx]\n","\n","        # Process this batch across all monthly files\n","        print(\"Reading and combining monthly files for batch...\")\n","        batch_data = process_location_batch(location_batch, data_dir)\n","\n","        # Process each location group in the batch\n","        print(\"Processing individual location groups...\")\n","        group_start = time.time()\n","        processed_groups = []\n","        for idx, ((lon, lat), group) in enumerate(batch_data.groupby(['longitude', 'latitude'])):\n","            if idx % 10000 == 0:\n","                print(f\"  Processing location {idx + 1}/{len(location_batch)} in current batch\")\n","            processed_group = process_single_group(group)\n","            processed_groups.append(processed_group)\n","\n","            # Update mapping with minimal necessary information\n","            location_mapping[(lon, lat)] = {\n","                'batch_file': f'batch_{batch_idx + 1:04d}.parquet',\n","                'mapping_idx': len(location_mapping) # NOTE: this idx is used by seq to reversely trace back to batch file(as seq loses the loc info already)\n","            }\n","\n","        group_duration = time.time() - group_start\n","        print(f\"Group processing completed in {group_duration:.2f} seconds\")\n","\n","        # Combine and save processed groups\n","        print(\"Saving batch results...\")\n","        save_start = time.time()\n","        batch_df = pd.concat(processed_groups, ignore_index=True)\n","\n","        # Validate before saving\n","        if validate_processed_batch(batch_df):\n","            batch_df.to_parquet(os.path.join(output_dir, f'batch_{batch_idx + 1:04d}.parquet'))\n","        else:\n","            print(\"ERROR: Batch validation failed!\")\n","            # Handle the error as needed\n","\n","        save_duration = time.time() - save_start\n","\n","        # Clean up\n","        del batch_data, processed_groups, batch_df\n","\n","        # Log batch completion\n","        batch_duration = time.time() - batch_iteration_start\n","        print(f\"\\nBatch {batch_idx + 1} completed:\")\n","        print(f\"  Total batch processing time: {batch_duration:.2f} seconds\")\n","        print(f\"  Average time per location: {batch_duration / (end_idx - start_idx):.2f} seconds\")\n","        print(f\"  Save time: {save_duration:.2f} seconds\")\n","        print(f\"{'='*80}\")\n","\n","    # Save location mapping\n","    print(\"\\nSaving location mapping...\")\n","    mapping_df = pd.DataFrame.from_dict(location_mapping, orient='index')\n","    mapping_df.index = pd.MultiIndex.from_tuples(mapping_df.index, names=['longitude', 'latitude'])\n","    mapping_df.to_parquet(os.path.join(output_dir, 'location_mapping.parquet'))\n","\n","    total_duration = time.time() - batch_start\n","    print(f\"\\nAll batches completed in {total_duration:.2f} seconds\")\n","    print(f\"Average time per batch: {total_duration / total_batches:.2f} seconds\")\n","    print(f\"Average time per location: {total_duration / total_locations:.2f} seconds\")\n","    print(f\"{'='*80}\\n\")\n","\n","def validate_location_groups(data_dir, file_pattern=PARQUET_PATTERN, output_dir=OUTPUT_TOP_PATH):\n","    \"\"\"\n","    Validate that all parquet files contain the same location groups as the cached version.\n","    Returns True if validation passes, False otherwise.\n","    \"\"\"\n","    print(f\"\\n{'='*80}\")\n","    print(f\"Starting location group validation at {datetime.now().strftime('%H:%M:%S')}\")\n","\n","    # Load cached locations\n","    cache_path = os.path.join(output_dir, 'all_locations_groups.csv')\n","    if not os.path.exists(cache_path):\n","        print(f\"Error: Cache file not found at {cache_path}\")\n","        return False\n","\n","    cached_locations = pd.read_csv(cache_path)\n","    cached_locations_set = set(\n","        zip(cached_locations['longitude'], cached_locations['latitude'])\n","    )\n","    print(f\"Loaded {len(cached_locations_set)} cached unique locations\")\n","\n","    # Get all parquet files\n","    parquet_files = glob.glob(os.path.join(data_dir, file_pattern))\n","    print(f\"Found {len(parquet_files)} parquet files to validate\")\n","\n","    validation_passed = True\n","    for file_path in parquet_files:\n","        file_start = time.time()\n","        print(f\"\\nValidating: {os.path.basename(file_path)}\")\n","\n","        # Read only location columns\n","        try:\n","            location_df = pd.read_parquet(file_path, columns=['longitude', 'latitude'])\n","            file_locations_set = set(\n","                zip(location_df['longitude'], location_df['latitude'])\n","            )\n","\n","            # Compare sets\n","            if file_locations_set != cached_locations_set:\n","                validation_passed = False\n","                missing_in_file = cached_locations_set - file_locations_set\n","                extra_in_file = file_locations_set - cached_locations_set\n","\n","                print(f\"ERROR: Location mismatch in {os.path.basename(file_path)}\")\n","                print(f\"  - File has {len(file_locations_set)} unique locations\")\n","                print(f\"  - Cache has {len(cached_locations_set)} unique locations\")\n","                if missing_in_file:\n","                    print(f\"  - {len(missing_in_file)} locations missing in file\")\n","                if extra_in_file:\n","                    print(f\"  - {len(extra_in_file)} extra locations in file\")\n","            else:\n","                print(f\"✓ Validation passed: {len(file_locations_set)} locations match\")\n","\n","            file_duration = time.time() - file_start\n","            print(f\"  Time taken: {file_duration:.2f} seconds\")\n","\n","        except Exception as e:\n","            validation_passed = False\n","            print(f\"ERROR: Failed to process {os.path.basename(file_path)}\")\n","            print(f\"Error details: {str(e)}\")\n","\n","    print(f\"\\n{'='*80}\")\n","    if validation_passed:\n","        print(\"✓ All files passed location group validation\")\n","    else:\n","        print(\"✗ Validation failed - see errors above\")\n","    print(f\"{'='*80}\\n\")\n","\n","    return validation_passed"],"metadata":{"id":"Fdqsognbf0uL","executionInfo":{"status":"ok","timestamp":1739981383198,"user_tz":300,"elapsed":11,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9_-a_EMzC9k","outputId":"fe34fbfa-8fd9-45b5-d957-f9f4225b914b","executionInfo":{"status":"ok","timestamp":1739986156052,"user_tz":300,"elapsed":4772853,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"source":["# Process the data\n","total_start = time.time()\n","print(f\"Starting data processing pipeline at {datetime.now()}\")\n","\n","# Process the data in batches\n","batch_process_locations(DATA_DIR, OUTPUT_PATH, batch_size=LOC_BATCH_SIZE)\n","\n","total_duration = time.time() - total_start\n","print(f\"\\nEntire processing pipeline completed in {total_duration:.2f} seconds\")\n","print(f\"Finished at {datetime.now()}\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting data processing pipeline at 2025-02-19 16:09:43.067945\n","\n","================================================================================\n","Starting batch processing at 16:09:43\n","Data directory: /content/drive/MyDrive/GEE_Exports/Alberta/2024/2024_8-10\n","Output directory: /content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed/2025\n","Batch size: 50000\n","================================================================================\n","\n","\n","Loading cached locations from /content/drive/My Drive/Colab Notebooks/wildfire/new_data/processed/all_locations_groups.csv\n","Loaded 767323 unique locations from cache\n","\n","Processing 767323 locations in 16 batches\n","================================================================================\n","\n","Batch 1/16\n","Processing locations 0 to 50000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 240.18 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 1 completed:\n","  Total batch processing time: 319.13 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 22.84 seconds\n","================================================================================\n","\n","Batch 2/16\n","Processing locations 50000 to 100000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 242.21 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 2 completed:\n","  Total batch processing time: 289.86 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 11.67 seconds\n","================================================================================\n","\n","Batch 3/16\n","Processing locations 100000 to 150000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 236.90 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 3 completed:\n","  Total batch processing time: 286.50 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 15.97 seconds\n","================================================================================\n","\n","Batch 4/16\n","Processing locations 150000 to 200000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 264.45 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 4 completed:\n","  Total batch processing time: 319.69 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 20.49 seconds\n","================================================================================\n","\n","Batch 5/16\n","Processing locations 200000 to 250000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 343.29 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 5 completed:\n","  Total batch processing time: 398.31 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 22.03 seconds\n","================================================================================\n","\n","Batch 6/16\n","Processing locations 250000 to 300000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 356.48 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 6 completed:\n","  Total batch processing time: 433.06 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 40.12 seconds\n","================================================================================\n","\n","Batch 7/16\n","Processing locations 300000 to 350000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 342.67 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 7 completed:\n","  Total batch processing time: 416.12 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 19.22 seconds\n","================================================================================\n","\n","Batch 8/16\n","Processing locations 350000 to 400000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 258.38 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 8 completed:\n","  Total batch processing time: 317.91 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 26.17 seconds\n","================================================================================\n","\n","Batch 9/16\n","Processing locations 400000 to 450000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 213.15 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 9 completed:\n","  Total batch processing time: 258.28 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 11.46 seconds\n","================================================================================\n","\n","Batch 10/16\n","Processing locations 450000 to 500000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 220.62 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 10 completed:\n","  Total batch processing time: 261.72 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 10.83 seconds\n","================================================================================\n","\n","Batch 11/16\n","Processing locations 500000 to 550000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 214.37 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 11 completed:\n","  Total batch processing time: 269.77 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 15.83 seconds\n","================================================================================\n","\n","Batch 12/16\n","Processing locations 550000 to 600000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 206.78 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 12 completed:\n","  Total batch processing time: 248.10 seconds\n","  Average time per location: 0.00 seconds\n","  Save time: 10.43 seconds\n","================================================================================\n","\n","Batch 13/16\n","Processing locations 600000 to 650000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 226.31 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 13 completed:\n","  Total batch processing time: 272.12 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 15.74 seconds\n","================================================================================\n","\n","Batch 14/16\n","Processing locations 650000 to 700000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 202.39 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 14 completed:\n","  Total batch processing time: 247.21 seconds\n","  Average time per location: 0.00 seconds\n","  Save time: 15.28 seconds\n","================================================================================\n","\n","Batch 15/16\n","Processing locations 700000 to 750000 (50000 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 50000 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/50000 in current batch\n","  Processing location 10001/50000 in current batch\n","  Processing location 20001/50000 in current batch\n","  Processing location 30001/50000 in current batch\n","  Processing location 40001/50000 in current batch\n","Group processing completed in 221.19 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 15 completed:\n","  Total batch processing time: 258.51 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 11.09 seconds\n","================================================================================\n","\n","Batch 16/16\n","Processing locations 750000 to 767323 (17323 locations)\n","Reading and combining monthly files for batch...\n","Processing batch of 17323 locations\n","Reading file: final_file-08_n5.parquet\n","Reading file: final_file-09_n5.parquet\n","Reading file: final_file-10_n5.parquet\n","Combining data from all files...\n","Processing individual location groups...\n","  Processing location 1/17323 in current batch\n","  Processing location 10001/17323 in current batch\n","Group processing completed in 106.13 seconds\n","Saving batch results...\n","Validation passed: No duplicates found in processed batch\n","\n","Batch 16 completed:\n","  Total batch processing time: 161.90 seconds\n","  Average time per location: 0.01 seconds\n","  Save time: 6.81 seconds\n","================================================================================\n","\n","Saving location mapping...\n","\n","All batches completed in 4772.28 seconds\n","Average time per batch: 298.27 seconds\n","Average time per location: 0.01 seconds\n","================================================================================\n","\n","\n","Entire processing pipeline completed in 4772.92 seconds\n","Finished at 2025-02-19 17:29:15.985930\n"]}]},{"cell_type":"code","source":["runtime.unassign()"],"metadata":{"id":"8mOujxjXxY6O","executionInfo":{"status":"ok","timestamp":1739986156252,"user_tz":300,"elapsed":197,"user":{"displayName":"Ben","userId":"16626926592568551176"}}},"execution_count":7,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}